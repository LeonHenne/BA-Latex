\section{Zielsetzung}

Daher soll im Rahmen dieser Arbeit untersucht werden, ob die Integrierung eines RL basierten Gegenspielers in einer Simulation die Umgebung so beeinflussen kann, dass die erlernten Verhaltensmodelle, welche im Kontext von RL oftmals als Policies referenziert werden, robuster agieren unter den veränderten dynamischen Bedingungen und alternativen deterministischen Gegenspielern im Testszenario. 

Dazu soll eine kompetitive Simulationsumgebung entwickelt werden, in welcher sich zwei konkurrierender Spieler in Form von Flugobjekten spielerisch gegenseitig bekämpfen.
In der Simulation werden folgende Policies in drei verschiedenen Szenarien trainiert.

\begin{itemize}
    \item Training mit regelbasiertem Gegenspieler unter gleichbleibenden Dynamikparametern
    \item Training mit RL basiertem Gegenspieler unter gleichbleibenden Dynamikparametern
    \item Training mit regelbasiertem Gegenspieler unter sich verändernden Dynamikparametern
\end{itemize}

Anschließend werden alle trainierten Policies in einer Reihe von Testszenarien untersucht.
Jedes Testszenario verfügt dabei über festgelegte sich vom Training unterscheidende Dynamikparameter und jeweils leicht unterschiedliche Handlungspräferenzen des deterministischen Gegenspielers.
Bei der Untersuchung werden jeweils die folgenden Variablen als Key Performance Indicator (KPI) betrachtet.
\begin{itemize}
    \item durchschnittlich erzielte Belohnung
    \item Varianz der Belohnungen
    \item Anzahl an unbeabsichtigten Abstürzen
\end{itemize}

Aus der Auswertung der Testszenarien kann der Effekt des RL basierten Gegenspielers auf die Robustheit mittels des Vergleichs mit dem regelbasierten Gegenspieler und der Domain Randomization evaluiert werden.


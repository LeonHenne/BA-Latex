\section{Zielsetzung}

Im Rahmen dieser Arbeit soll untersucht werden, ob die Integrierung eines RL basierten Gegenspielers in einer Simulation die Umgebung so beeinflussen kann, dass die erlernten Verhaltensmodelle, welche im Kontext von RL oftmals als Policies referenziert werden, robuster agieren unter den veränderten dynamischen Bedingungen und alternativen deterministischen Gegenspielern im Testszenario. 

Dazu soll eine kompetitive Simulationsumgebung entwickelt werden, in welcher sich zwei konkurrierender Spieler in Form von Flugobjekten spielerisch gegenseitig bekämpfen.
In der Simulation werden folgende Policies in drei verschiedenen Szenarien trainiert.

\begin{itemize}
    \item Training mit regelbasiertem Gegenspieler unter gleichbleibenden Dynamikparametern
    \item Training mit RL basiertem Gegenspieler unter gleichbleibenden Dynamikparametern
    \item Training mit regelbasiertem Gegenspieler unter sich verändernden Dynamikparametern
\end{itemize}

Anschließend werden alle trainierten Policies in einem Testszenario untersucht.
Das Testszenario verfügt dabei über festgelegte sich vom Training unterscheidende Eigenschaften, wie die Start- und Zielpositionen.
Außerdem wird ein deterministischen Gegenspieler unter im Gegensatz zum Training, veränderten Handlungspräferenzen eingesetzt.
Bei der Untersuchung werden jeweils die folgenden Variablen als Key Performance Indicator (KPI) betrachtet.
\begin{itemize}
    \item kumulierte erzielte Belohnung
    \item Varianz der Belohnungen
    \item Anzahl an unbeabsichtigten Abstürzen
\end{itemize}

Aus der Auswertung des Testszenarios kann der Effekt des RL basierten Gegenspielers auf die Robustheit evaluiert werden, indem die Leistungsdiskrepanz zwischen Trainings- und Testszenario mit der des regelbasierten Gegenspieler und der der Domain Randomization verglichen wird.


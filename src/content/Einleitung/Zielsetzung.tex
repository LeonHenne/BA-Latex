\section{Zielsetzung}

Im Rahmen dieser Arbeit soll eine Simulationsumgebung entwickelt werden, in welcher sich RL basierter Gegenspieler und deterministische Gegenspieler integrieren lassen.
Anschließend ist zu untersuchen, wie sich die Wahl des Trainingsgegenspielers auf das Leistungsverhalten der RL Modelle Strategien, im Kontext von RL oftmals als Policies referenziert, unter verändertem Testszenario auswirkt.

Dazu soll eine kompetitive Simulationsumgebung entwickelt werden, in welcher sich zwei konkurrierender Spieler in Form von Flugobjekten spielerisch gegenseitig bekämpfen.
In der Simulation werden folgende Policies in drei verschiedenen Szenarien optimiert.

\begin{itemize}
    \item Training mit regelbasiertem Gegenspieler unter gleichbleibenden Dynamikparametern
    \item Training mit RL basiertem Gegenspieler unter gleichbleibenden Dynamikparametern
    \item Training mit regelbasiertem Gegenspieler unter sich verändernden Dynamikparametern
\end{itemize}

Anschließend werden alle trainierten Policies in einem Testszenario untersucht.
Das Testszenario verfügt dabei über festgelegte, sich vom Training unterscheidende Eigenschaften, wie Flugbahnen, oder Start- und Zielpositionen.
Außerdem wird ein deterministischer Gegenspieler unter im Gegensatz zum Training, veränderten Handlungspräferenzen eingesetzt.
Bei der Untersuchung werden jeweils die folgenden Variablen als Key Performance Indicator (KPI) betrachtet.
\begin{itemize}
    \item kumulierte erzielte Belohnung
    \item Varianz der Belohnungen
    \item Anzahl an Misserfolgen
\end{itemize}

Durch die Auswertung des Testszenarios kann der Effekt des RL basierten Gegenspielers auf die Robustheit evaluiert werden, indem die Leistungsdiskrepanz im Testszenario zwischen den Strategien im Training mit RL- und regelbasierten Gegenspielers- und der Domain Randomization verglichen wird.


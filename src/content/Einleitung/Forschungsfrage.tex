\section{Forschungsfrage}

Aus der beschriebenen Problemstellung und der für den Rahmen dieser Arbeit festgelegten Zielsetzung ergibt sich folgende Forschungsfrage: 

\textit{Inwiefern kann durch den Einsatz eines mittels RL trainierten Gegenspielers die Robustheit einer optimierten Policy verbessert werden?}
%von Domain Adaptation Methoden oder im Vergleich zu existierenden Methoden

Zur Beantwortung der Forschungsfrage werden folgende Hypothesen aufgestellt und im Rahmen der Arbeit untersucht:

\textbf{Hypothese 1:}
\textit{Die im Testszenario erzielte kumulierte Belohnung ist unter Verwendung der Policy aus dem Training mit RL basiertem Gegenspieler signifikant und zuverlässig höher, als die Policy aus dem Training mit regelbasiertem Gegenspieler.}
%\textit{Die durchschnittlich erzielte Belohnung ist unter Verwendung der Policy aus dem Training mit RL basiertem Gegenspieler signifikant und zuverlässig höher als die Policy aus dem Training mit veränderten Dynamikparametern.}

\textbf{Hypothese 2:}
\textit{Die Varianz der im Testszenario erzielten kumulierten Belohnung ist unter Verwendung der Policy aus dem Training mit RL basiertem Gegenspieler signifikant und zuverlässig geringer, als die Policy aus dem Training mit regelbasiertem Gegenspieler.}
%\textit{Die Varianz der Belohnung ist unter Verwendung der Policy aus dem Training mit RL basiertem Gegenspieler signifikant und zuverlässig geringer als die Policy aus dem Training mit veränderten Dynamikparametern.}

\textbf{Hypothese 3:}
\textit{Die im Testszenario erreichte Anzahl von Misserfolgen ist unter Verwendung der Policy aus dem Training mit RL basiertem Gegenspieler signifikant und zuverlässig geringer, als die Policy aus dem Training mit regelbasiertem Gegenspieler.}
%\textit{Die Anzahl von Abstürzen ist unter Verwendung der Policy aus dem Training mit RL basiertem Gegenspieler signifikant und zuverlässig geringer als die Policy aus dem Training mit veränderten Dynamikparametern.}


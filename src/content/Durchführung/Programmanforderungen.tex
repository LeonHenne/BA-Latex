\section{Programmanforderungen}

Für die Durchführung des Laborexperiments wird ein Softwareprogramm implementiert, welches in der Lage ist, die zuvor beschriebene Methodik umzusetzen.
In diesem Kapitel werden die dafür bestehenden technischen und funktionalen Anforderungen genauer thematisiert. 

% allgemeine Programmanforderung
Eine allgemeine Anforderung liegt in der Wahl einer Entwicklungssprache, welche die Entwicklung der Simulation anhand von Softwareframeworks und zugleich die Integration mit Algorithmen des verstärkenden Lernens erlaubt.
Die Entwicklungssprache und verwendete Softwarebibliotheken sollten einer möglichst aktuellen Version entsprechen, und deren Einsatz mit ihrer Version dokumentiert sein.
Durch die Dokumentierung kann ein gleiches Abbild der Softwareumgebung auf anderen Instanzen eines Betriebssystems eingerichtet werden. 

\subsection{Anforderungen der Simulationsumgebung}

Die übergeordnete Hauptaufgabe der Simulation ist die Generierung von Trainingsdaten für die Optimierung von Strategien mittels RL.
Die Qualität der Simulation und besonders der Grad des Realismus spielen dabei besonders für die spätere Auswertung der Robustheit einzelner Policies eine wichtige Rolle.
In diesem Abschnitt wird die Hauptaufgabe in mehrere Anforderungen unterteilt, welche im Entwicklungsprozess umzusetzen sind.

% Programmanforderung der Gymnasium Schnittstelle
Aus der Betrachtung des aktuellen Stands der Forschung und der Praxis für die Entwicklung von Simulationen geht hervor, dass besonders das OpenAI Gym Framework bzw. dessen Nachfolger Gymnasium in diesem Kontext ein wichtiger Bestandteil ist.
Das Gymnasium Framework definiert die wichtigsten Funktionsschnittstellen, welche zur Entwicklung der Simulation im Rahmen dieser Arbeit zu implementieren sind.

Eine der Funktionen beschreibt die \textbf{Initialisierung} der eigenen Umgebungsklasse, welche von der Umgebungsklasse der Gymnasium-Bibliothek erbt.
Initial sind darin für die Simulation die Startpositionen der verteidigenden Drohne und der angreifenden Drohne sowie der anzugreifende Zielpunkt anzugeben.
Die Simulation soll eine zufällige Platzierung der Drohnen und des Zielpunktes im Raum ermöglichen, jedoch ausschließlich unter der Bedingung, dass die direkte Strecke zum Zielpunkt für die verteidigende Drohne kürzer ist, als für die angreifende Drohne.
Zur Erfüllung dieser Anforderung muss eine Wahrscheinlichkeitsfunktion für die Zielpunktkoordinaten sowie je Drohne, für die drei Startkoordinaten X, Y und Z bestimmt werden.
Die Randomisierung dieser Koordinaten soll über eine Variable an- und abgeschaltet werden können, sodass einfach zwischen Trainings- und Testszenario gewechselt werden kann.
Als Spielgebiet wird eine flache feste Ebene auf der Höhe Null eingesetzt, über dessen der drei dimensionale Raum unbegrenzt in X-, Y- und Z-Richtung zur Verfügung steht.

Eine weitere Funktion beinhaltet die Definition des \textbf{Beobachtungsraums}, also welche Informationen für den Agenten wahrnehmbar sind. 
In der beschriebenen Simulation soll der Agent die aktuelle Position und Richtungsgeschwindigkeit der Drohne wahrnehmen.
Jede dieser Eigenschaften wird von der verteidigenden Drohne und zusätzlich von der angreifenden Drohne wahrgenommen. 
Insgesamt stellt der Beobachtungsraum somit ein Datenobjekt dar, welches alle Informationen wie Koordinaten, Geschwindigkeiten und Drehzahlen beinhalte.

Neben dem Beobachtungsraum muss auch ein \textbf{Handlungsraum} festgelegt werden, welcher die Steuerung der verteidigenden Drohne umfassen soll.
Hierbei soll durch den Agenten die Geschwindigkeit und ihre Richtung in Form einer vier elementigen Liste vorgegeben werden.
Auch die regelbasiert gesteuerten Drohnen müssen damit eine dieser Form entsprechende Aktion erzeugen.
Anschließend besteht die Anforderung, diese Aktionen in einen konkreten Einfluss auf den Zustand des Quadrokopters zu übersetzen.

Führt der Agent eine Aktion seines Handlungsraums aus, wird die Implementierung eines \textbf{Zeitschritts} der Simulation benötigt.
In dieser muss deklariert sein, wie sich die gewählte Aktion auf den Zustand der Drohnen auswirkt.
Während jedes Zeitschritts der Simulation sind dabei die Kriterien zum Zurücksetzen der Simulation zu prüfen.

Im Anwendungsbereich dieser Arbeit ist die Simulation zurückzusetzen, sobald das Abfangen der angreifenden Drohne erfolgt ist, oder eine der Drohnen in Kontakt mit dem Untergrund kommt.
Das Gymnasium Framework definiert die \textbf{Rücksetzfunktion}, welche den initialen Simulationszustand herstellt, sobald ein Kriterium erfüllt ist.
Durch die Schrittfunktion ist abschließend die neue Wahrnehmung der Simulation, die Erfüllung des Rücksetzkriteriums und der Belohnungswert zurückzugeben.

Zur Berechnung einer entsprechenden Belohnung, für die Ausführung der gewählten Aktion, ist eine \textbf{Belohnungsfunktion} einzusetzen.
Diese muss anhand des neuen Simulationszustandes Kennzahlen ermitteln und gewichten und dadurch eine numerische Bewertung der neuen Situation vornehmen.
Hinsichtlich der Maximierung des kumulierten Belohnungswertes optimiert der Agent mittels des RL Algorithmus seine Aktion in Abhängigkeit der Simulation.
Die Anforderungen dieser Optimierung der Aktionen werden in der nachfolgenden Sektion detailliert betrachtet.

\subsection{Anforderungen des Optimierungsverfahrens}

Das Optimierungsverfahren trägt die Aufgabe zur Bestimmung der bestmöglichen Aktion zu jedem Zeitschritt der Simulation.
Dabei ist die Strategie zur Steuerung des Quadrokopters mittels eines RL-Algorithmus zu optimieren.
%Hierfür soll ein RL-Algorithmus des aktuellen Stands der Forschung und der Praxis eingesetzt werden.
Da die eigene Implementierung des RL-Algorithmus nicht im Fokus dieser Arbeit liegt, werden frei verfügbare Softwarebibliotheken verwendet.
Eine Bibliothek, welche eine Vielzahl an RL-Algorithmen beinhaltet, ist Stable-Baselines3 nach \cite[]{Raffin.2021}.
Die Softwarebibliothek integriert weitere Bibliotheken wie Tensorboard zum Messen von Trainings- und Evalutationskennzahlen, was die Durchführung der Forschungsmethodik unterstützt.
Weiterhin ist eine umfangreiche Dokumentation zur Anwendung von RL-Algorithmen in Verbindung mit dem Gymnasium Framework vorhanden, was die Umsetzung des Trainingsprozesses in Verbindung mit der Simulationsumgebung erleichtert.
Aus der Softwarebibliothek Stable-Baselines3 soll der PPO-Algorithmus zum Optimieren je einer Policy in jedem Trainingsszenario eingesetzt werden. 

Der Trainingsprozess beinhaltet im ersten Schritt die Bestimmung der unabhängigen Variablen. 
Im Kontext dieser Arbeit sind die die Eigenschaften der Simulation nach der Wahl des Trainingsszenarios in der Simulation einzustellen.
Alle Eigenschaften der Szenarios sollen über die Veränderung weniger Parameter als zentrale Schnittstelle im Programm auswählbar sein.
Anschließend sind diejenigen Metriken anzugeben, welche im Trainingsprozess erfasst werden sollen.
Diese beinhalten die durchschnittliche Episodenlänge, die kumulierte Belohnung sowie die Erfolgsrate des Zusammenstoßes beider Drohnen, über alle im Trainingsprozess evaluierten Strategien.
Zur Erfassung dieser Trainingsmetriken gilt es, die Integration mit der Tensorboard Bibliothek zu nutzen.
Daraufhin ist das Training über die dafür vorgesehene Funktion des RL-Modells zu starten und nach dessen Ablauf das optimierte Modell zu speichern.
Das Speicherformat ist so zu wählen, dass gespeicherte Modelle zur Auswertung in das Testszenario geladen werden können.

% Imitation Learning
Aufgrund des hohen dreidimensionalen Aktionsraums, welcher zu jedem Zeitschritt zu optimieren ist, soll vor dem beschriebenen Trainingsprozess ein Training durch IL eingesetzt werden.
Im IL Prozess ist ein Modell dahingehend zu optimieren, eine vorgeschriebene regel-basierte Strategie zur Steuerung des Quadrokopters nachzuahmen. 
Die Policies der später mittels RL zu optimierenden Quadrokopter werden demnach zunächst regelbasiert als Experte definiert. 
Daraufhin ist es das Ziel, die Gewichte eines neuronalen Netzes mithilfe eines IL-Algorithmus wie z.B. Behavioral Cloning so anzupassen, dass diese bestmöglich die regelbasierte Strategie widerspiegeln.
Anschließend sind die in diesem Prozess erlernten Gewichte in ein RL-Modell Softwareobjekt zu transferieren und mittels des gewählten RL-Algorithmus feinzujustieren.
Durch diesen Transfer soll das Training der RL-Modelle beschleunigt werden, da die Startgewichte bereits näher der optimalen Strategie liegen als zufällig gewählte Gewichte.

\subsection{Anforderungen des Laborexperiments}

Mithilfe der programatischen Implementierung des Laborexperiments sollen die notwendigen Messdaten erhoben, die Forschungshypothesen evaluiert, und die Forschungsfrage beantwortet werden.
Dazu ist es notwendig, die beschriebenen Metriken, wie die kumulierte Belohnung deren Varianz und die Anzahl an Misserfolgen, für alle Policies im Testszenario zu erfassen.
Das Testszenario wird durch eine Instanz der Simulationsumgebung repräsentiert, die zum Training veränderte Parameter beinhaltet.

Die Abweichung der Domäne zu den Trainingsszenarien wird durch die zusätzliche Randomisierung des Zielpunktes und durch die Verbesserung des regelbasierten Gegenspielers erzielt. 
Die Verteilung des Zielpunktes soll dabei ähnlich zu den Startkoordinaten so gewählt sein, dass sich die verteidigende Drohne näher an ihr befindet.
Das Abfangen eines direkten Anflugs der anzugreifenden Drohne bleibt dadurch möglich.
Im Testszenario soll die Verbesserung der anzugreifenden Drohne aus einer parabelförmigen anstatt geraden Fluglinie zum Zielpunkt bestehen.
Damit die Validität der Messdaten gegeben ist, muss sichergestellt sein, dass alle Strategien den gleichen Testbedingungen und demnach denselben zufälligen Start- und Zielpunkten unterliegen.
Hierfür ist ein Zufallsstartwert zu übergeben, worunter bei jedem Programmstart die gleichen zufälligen Testbedingungen hergestellt werden.
Anschließend ist jede Policy in mehrfachen verschiedenen Episoden auszuführen und deren jeweilige Leistungsverhalten zu messen.

Innerhalb eines Testszenarios sind zu jedem Zeitschritt die aktuelle Belohnung zu speichern und zu messen, ob ein Misserfolg der Drohne vorliegt.
Beide Variablen folgen dem Format einer Zeitreihe über den Verlauf der Testsimulation.
Das Programm zur Auswertung muss nach 100-facher Ausführung der Policy im Testszenario die Kennzahlen der durchschnittlichen Belohnung und dessen Varianz berechnen.
Zur Dokumentation und für die weitere Auswertung mittels statistischer Tests sind die Messdaten zu speichern.

Durch das Laden der Messdaten aus Dateien in entsprechende Datenobjekte liegen alle Metriken zur Bestimmung des statistischen Tests vor.
Anschließend sind die Daten durch das Auswertungsprogramm, mittels der in der Forschungsmethodik beschriebenen Signifikanztests auf ihre Verteilung hin zu untersuchen.
In Abhängigkeit der vorliegenden oder nicht vorliegenden Normalverteilung ist entsprechend der in der Forschungsmethodik beschriebene Test umzusetzen.

Die genaue Implementierung dieser Tests soll durch zusätzliche Softwarebibliotheken bestimmt sein.
Anhand der Testergebnisse sind durch das Programm die H0 und H1 Hypothesen zu evaluieren und darauf aufbauend die Forschungshypothesen zu bestätigen oder zu verwerfen.
Zusätzlich zu den Messdaten der Testszenarien sind die Ergebnisse der Signifikanztests zu speichern.
Schlussendlich sollen in einer Tabelle zu jeder Forschungshypothese die Ergebnisse der statistischen Tests angeführt werden.
Durch die gesamtheitliche Betrachtung der Robustheitsmerkmale kann final die Forschungsfrage beantwortet werden.

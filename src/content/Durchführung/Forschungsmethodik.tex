Anschließend an die Diskussion des aktuellen Stands der Forschung und der Praxis wird im Rahmen dieses Kapitels die Forschungsmethodik durchgeführt, um folgende Forschungsfrage zu beantworten:

\textit{Inwiefern kann durch den Einsatz eines mittels RL trainierten Gegenspielers die Robustheit der gelernten Strategie verbessert werden?}

Die in der Forschungsfrage formulierte These stellt ein Weg einer mögliche Verbesserungen des Sim-to-Real Problems dar, welchen es zu Überprüfen gilt.
Für diese Überprüfung wird ein Laborexperiment nach \cite[]{Recker.2021} durchgeführt, indem die Robustheit der erlernten Strategien unter deterministischem und trainiertem Gegenspieler betrachtet wird. 
Die quantitative Auswertung mittels des Laborexperiments kann hierbei besonders die Ursache-Wirkungs Beziehung zwischen dem gewählten Trainingsszenario und der erzielten Robustheit untersuchen.

In der ersten Sektion dieses Kapitels wird auf den Aufbau des Laborexperiments eingegangen.
Dabei werden grundlegende Trainingsdaten sowie zu untersuchende Hypothesen, ausgeübte Einflüsse und gemessene Metriken beschrieben.
Anschließend werden daraus in der nächsten Sektion resultierende Anforderungen für die Entwicklung der Simulation und der Testumgebung abgeleitet. 
Die dritte Sektion setzt daraufhin die beschriebenen Anforderungen um, und erläutert die endgültige Implementierung der Simulation, der Testdatenerhebung und deren Auswertung.

\section{Erläuterung der Forschungsmethodik}

Innerhalb des Laborexperiments soll die Beziehung zwischen dem Trainingsszenario als Ursache und der Leistungsrobustheit als Wirkung betrachtet werden.
Dabei sind zunächst Strategien durch verstärkendes Lernen unter unterschiedlichen Szenarien zu optimieren. 
Anschließend werden die trainierten Policies in mehreren Testszenarien einer veränderten Simulation ausgeübt und währenddessen verschiedene Metriken der Strategieleistung betrachtet.
Kein Teil der umzusetzenden Forschungsmethodik ist daher die Anwendung der simulationsbasierten Strategien zur Steuerung von Quadrokoptern in der echten Welt. 
Weiterhin wird nicht die Ergebnisabhängigkeit zu Faktoren untersucht, wie der Wahl der Simulationsumgebung, der Physik-Engine oder des Abstraktionsniveaus.
Die Auswahl jener Aspekte wird unter den Gesichtspunkten der Softwarearchitektur getroffen, dessen Anforderungen und Implementierung in nachfolgenden Sektionen behandelt wird.

\subsection{Beschreibung der Simulationsumgebung}

Beginnend mit dem Training der Policies durch verstärkendes Lernen, wird hierfür anders als bei überwachtem und unüberwachtem Lernen kein unmittelbar vorliegender Datensatz benötigt.
Anstelle dessen basiert das Training auf der vollständigen oder teilweisen Wahrnehmung einer Lernumgebung, welche den lernenden Agenten für ausgeführte Aktionen positiv oder negativ belohnt.
Wie auch in der Anleitung erwähnt wird im Kontext dieser Arbeit die Simulation von Quadrokoptern dafür verwendet, die Lernumgebung und damit die Trainingsdaten für die Algorithmen des RL zur Verfügung zu stellen.
Die Simulation von Quadrokoptern stellt ein hochdynamisches Anwendungsgebiet dar, bei dem von einer hohen Diskrepanz zwischen der echten und simulierten Welt ausgegangen werden kann. 
Die Simulationsumgebung stellt grundsätzlich ein Szenario dar, in welchem zwei verschiedene Drohnen kompetitiv gegeneinander agieren.
Das Ziel einer Drohne ist es zu einem festgelegten Punkt hinter der zweiten Drohne zu gelangen, währenddessen die zweite Drohne versucht die angreifende Drohne auf seinem Weg abfzuangen. 
Die verteidigende Drohne verwendet dafür das Mittel einer bewussten Kollision und wird zu Beginn näher an dem Zielpunkt platziert als die angreifende Drohne. 
Initial können die Drohnen zwar zufällig im Raum platziert werden, jedoch ausschließlich unter der Bedingung, dass die direkte Strecke für die verteidigende Drohne kürzer ist, als für die angreifende Drohne.
Als Spielgebiet wird eine flache feste Ebene auf der Höhe Null eingesetzt, über dessen der drei dimensionale Raum unbegrenzt in X-, Y-, und Z-Richtung zur Verfügung steht.
In der Simulation agiert stets ausschließlich ein Agent zur Steuerung des verteidigenden Quadrokopters, welcher während des Trainings seine Strategie zum Abfangen der gegnerischen Drohne mittels RL-Algorithmus optimiert. 
Die angreifende Drohne verfolgt während der Trainings- und Testphasen des Agenten ausschließlich deterministisch vorgeschriebene Strategien, welche sich nicht zugleich optimieren.

\subsection{Erläuterung der Teststzenarien}

Durch die erläuterte Simulation werden insgesamt für das Laborexperiment vier Strategien mit RL erlernt.
Jede dieser Policies wird dabei in einer der nachfolgenden Simulationsszenarien optimiert, dessen Auswahl eine der unabhängige Variablen des Laborexperiments darstellt.
\begin{enumerate}
    \item Das erste Szenario beinhaltet das Training der zu verteidigenden Drohne gegen eine angreifende Drohne, welche eine deterministische regelbasierte Strategie ausführt.
    \item Im zweiten Szenario wird eine deterministische Strategie für die anzugreifende Drohne mittels RL optimiert, während die verteidigende Drohne anhand ihrer zuvor optimierten Strategie agiert.
    \item Anschließend enthält das dritte Szenario das Training einer Verteidigungsstrategie im kompetitiven Spiel mit der anzugreifenden Drohne unter zuvor optimierter Policy.
    \item Zum Abschluss wird das erste Szenario mit regelbasiertem Gegenspieler unter DR, also unter der Randomisierung dynamischer Parameter, als viertes Szenario wiederholt. 
\end{enumerate}

\subsection{Messung des Robustheit von RL Policies}

Nach den Trainingsphasen werden einzelne Policies in mehreren vom Training abweichenden Testszenarien ausgeführt und deren Leistungsverhalten gemessen.
Die in den Testszenarien betrachteten abhängigen Variablen sind die durchschnittlich erzielte Belohnung, dessen Varianz sowie die Anzahl an unbeabsichtigten Abstürzen.
Die erzielte Belohnung und dessen Varianz stellen wie bereits beschrieben in der Forschung wichtige Kenngrößen dar, um die Robustheit von RL Policies zu bestimmen.
Die Metrik der Anzahl von unbeabsichtigten Abstürzen spiegelt im behandelten Anwendungsfall zusätzlich das Fehlschlagen der trainierten Strategie wider.
Mit der Auswahl der Metriken werden die Strategieeigenschaften der maximalen Strategieleistung, der Leistungsabweichung und der Strategiesicherheit deutlich.
Aus der Abweichung zwischen dem Leistungsverhalten während des Trainings und während des Tests, soll so die Robustheit von RL Policies erkennbar und messbar gestaltet werden.
Der Fokus liegt dabei auf der Messung der Robustheit von den optimierten Strategien aus Szenario eins, zwei und vier.
Aus dem Vergleich der Robustheit zwischen Strategie eins und zwei kann eine mögliche Verbesserung abgeleitet werden.
Anschließend kann der erzielte Effekt mit der Abweichung zwischen eins und vier verglichen, und so aktuell verwendete Methoden zur Erhöhung der Robustheit einbezogen werden.

\subsection{Auswertung mittels statistischer Tests}

Durch den Vergleich einzelner Strategien mittels dieser Methodik werden im Laborexperiment die zu Beginn der Arbeit aufgestellten Hypothesen auf ihre Gültigkeit untersucht.
Die nachfolgend angeführten Hypothesen beinhalten wie im vorherigen Kapitel diskutiert, verschiedene Aspekte der Robustheit von RL Strategien, welche in späteren Abschnitten einzeln ausgewertet werden.
\begin{enumerate}
    % verschiedene Aspekte: max. Reward, Stabilität des Rewards, Fehlschlag von Strategien
    \item \textit{Die in den Testszenarien durchschnittlich erzielte Belohnung ist unter Verwendung der Policy aus dem Training mit RL basiertem Gegenspieler signifikant und zuverlässig höher als die Policy aus dem Training mit regelbasiertem Gegenspieler.}
    \item \textit{Die Varianz der in den Testszenarien erzielten Belohnung ist unter Verwendung der Policy aus dem Training mit RL basiertem Gegenspieler signifikant und zuverlässig geringer als die Policy aus dem Training mit regelbasiertem Gegenspieler.}
    \item \textit{Die in den Testszenarien erreichte Anzahl von unbeabsichtigten Abstürzen ist unter Verwendung der Policy aus dem Training mit RL basiertem Gegenspieler signifikant und zuverlässig geringer als die Policy aus dem Training mit regelbasiertem Gegenspieler.}
\end{enumerate}

Jede der drei oben genannten Behauptungen wird in zwei statistischen Tests ausgewertet.
Dabei wird zum einen ein Test auf Ungleichheit und zum anderen ein Test zum Verbesserungseffekt durch RL basierten Gegenspieler durchgeführt.
Die Thesen zur ungleichen oder besseren Metrik durch trainierten Gegenspieler werden in den Signifikanztests als H0 Hypothese eingesetzt.
Die entsprechenden Gegenhypothesen formulieren jeweils die gegensätzliche Annahme mit regelbasiertem Gegenspieler. 
Fasst man dieses Aufbau der statistischen Signifikanztests zusammen können folgende Testhypothesen festgelegt werden. 

\textbf{Ungleichheit des ersten Messwerts}
\begin{itemize}
    \item \textbf{Hypothese H0:} \textit{Die durchschnittlich erzielte Belohnung im dritten Testszenarios ist signifikant unterschiedlich zu der des ersten Testszenarios.}
    \item \textbf{Hypothese H1:} \textit{Die durchschnittlich erzielte Belohnung im dritten Testszenarios ist signifikant gleich zu der des ersten Testszenarios.}
\end{itemize}

\textbf{Verbesserung des ersten Messwerts}
\begin{itemize}
    \item \textbf{Hypothese H0:} \textit{Die durchschnittlich erzielte Belohnung im dritten Testszenarios ist signifikant höher zu der des ersten Testszenarios.}
    \item \textbf{Hypothese H1:} \textit{Die durchschnittlich erzielte Belohnung im dritten Testszenarios ist signifikant geringer zu der des ersten Testszenarios.}
\end{itemize}

\textbf{Ungleichheit des zweiten Messwerts}
\begin{itemize}
    \item \textbf{Hypothese H0:} \textit{Die Varianz der erzielten Belohnung im dritten Testszenarios ist signifikant unterschiedlich zu der des ersten Testszenarios.}
    \item \textbf{Hypothese H1:} \textit{Die Varianz der erzielten Belohnung im dritten Testszenarios ist signifikant gleich zu der des ersten Testszenarios.}
\end{itemize}

\textbf{Verbesserung des zweiten Messwerts}
\begin{itemize}
    \item \textbf{Hypothese H0:} \textit{Die Varianz der erzielten Belohnung im dritten Testszenarios ist signifikant geringer zu der des ersten Testszenarios.}
    \item \textbf{Hypothese H1:} \textit{Die Varianz der erzielten Belohnung im dritten Testszenarios ist signifikant höher zu der des ersten Testszenarios.}
\end{itemize}

\textbf{Ungleichheit des dritten Messwerts}
\begin{itemize}
    \item \textbf{Hypothese H0:} \textit{Die erreichte Anzahl von unbeabsichtigten Abstürzen ist im dritten Testszenarios ist signifikant unterschiedlich zu der des ersten Testszenarios.}
    \item \textbf{Hypothese H1:} \textit{Die erreichte Anzahl von unbeabsichtigten Abstürzen ist im dritten Testszenarios ist signifikant gleich zu der des ersten Testszenarios.}
\end{itemize}

\textbf{Verbesserung des dritten Messwerts}
\begin{itemize}
    \item \textbf{Hypothese H0:} \textit{Die erreichte Anzahl von unbeabsichtigten Abstürzen ist im dritten Testszenarios ist signifikant geringer zu der des ersten Testszenarios.}
    \item \textbf{Hypothese H1:} \textit{Die erreichte Anzahl von unbeabsichtigten Abstürzen ist im dritten Testszenarios ist signifikant höher zu der des ersten Testszenarios.}
\end{itemize}

Die Auswahl des Signifikanztests wird anhand der Wahrscheinlichkeit einer vorliegenden Normalverteilung vorgenommen.
Kann nach einem Kolmogorov- oder Shapiro- Test zu einem Signifikanzniveau von 10\% eine Normalverteilung angenommen werden, lässt sich zut Überprüfung der Hypothesen ein T-Test einsetzten.
Ist keine Normalverteilung gegeben, wird ein Mann-Whitney U Test verwendet.
Liegt schlussendlich der P-Wert des T- oder Mann-Whitney U Signifikanztests unter 10\% wird die H0 Hypothese angenommen und die Abweichung in den Robustheitsdaten oder deren Verbesserung als signifikant betrachtet.
Zeigen die statistischen Signifikanztests, dass eine Metrik sich durch die Auswahl des Trainingsszenarios verbessert, wird die Abweichung als signifikant wahrgenommen, und die Hypothese bestätigt.
Wird unter allen untersuchten Hypothese mindestens eine angenommen, und zugleich keine Hypothese aufgrund einer Verschlechterung der Metrik abgelehnt, kann die Forschungsfrage damit beantwortet werden, dass ein Training von RL Policies mittels trainiertem Gegenspieler die Robustheit erhöhen kann.

\section{Programmanforderungen}

Für die Durchführung des Laborexperiments ist ein Softwareprogramm anzufertigen, welches in der Lage ist, die zuvor beschriebene Methodik umzusetzen.
In diesem Kapitel werden die dafür bestehenden technischen Anforderungen genauer untersucht und thematisiert. 

\subsection{Anforderungen der Simulationsumgebung}

Die übergeordnete Hauptaufgabe der Simulation ist die Generierung von Trainingsdaten für die Optimierung von Strategien mittels RL.
In diesem Abschnitt wird diese Anforderung zu mehreren kleineren Bedingungen getrennt, welche im Entwicklungsprozess umzusetzen sind.

Eine der ersten Bedingungen ist die Wahl einer Entwicklungssprache, welche die Entwicklung der Simulation anhand von Softwareframeworks, und zugleich die Integration mit Algorithmen des verstärkenden Lernens erlaubt.
Die Entwicklungssprache und verwendete Softwarebibliotheken sollten einer möglichst aktuellen Version entsprechen, und deren Einsatz mit ihrer Version dokumentiert sein.
Durch die Dokumentierung kann ein gleiches Abbild der Softwareumgebung auf anderen Instanzen eines Betriebssystems eingerichtet werden. 

Aus der Betrachtung des aktuellen Stands der Forschung und der Praxis für die Entwicklung von Simulationen geht hervor, dass besonders das OpenAI Gym Framework bzw. dessen Nachfolger Gymnasium in diesem Kontext ein wichtiger Bestandteil ist.
Das Gymnasium Framework definiert die wichtigsten Funktionsschnittstellen, welche zur Entwicklung von Simulationen zu implementieren sind.
Eine dieser Funktionen beinhaltet die Definition des Beobachtungsraums, also welche Informationen für den Agenten Wahrnehmbar sind. 
In der beschriebenen Simulation soll der Agent die Position, die Ausrichtung, die dreidimensionale Geschwindigkeit der Drohne und die Geschwindigkeit der Rotatoren wahrnehmen.
Jede diese Eigenschaften wird von der verteidigenden Drohne und zusätzlich von der angreifenden Drohne wahrgenommen. 
Neben dem Beobachtungsraums des Agenten muss auch sein Handlungsraum festgelegt werden, welcher die Steuerung der verteidigenden Drohne umfassen soll.
Hierbei soll durch den Agenten die Geschwindigkeit und ihre Richtung durch den Agenten vorgegeben werden.
Anschließend besteht die Anforderung diese Kenngrößen in konkrete Drehzahlen der einzelnen Rotatoren zu übersetzen.

Führt der Agent eine Aktion seines Handlungsraums aus, wird die Implementierung eines Zeitschritts der Simulation benötigt.
In dieser muss deklariert sein, wie sich die gewählte Aktion auf den Zustand der Drohnen auswirkt.
Die Funktion berechnet daraus die neue Erscheinungsform der Simulation.
Zusätzlich zum neuen Zustand wird ein Belohnungswert an den Agenten übergeben.
Zur Berechnung einer adequaten Belohnung für die Ausführung der gewählten Aktion ist eine Belohnungsfunktion einzusetzen.
Diese muss anhand der Zustandsvariablen und deren Gewichtung eine numerische Bewertung der neuen Situation vornehmen.
Der Agent bedarf dabei einen Algorithmus des verstärkenden Lernens, welcher die Aktionen hinsichtlich der Maximierung des Belohnungswertes ausrichtet. 
Während jedes Zeitschritts der Simulation sind die Kriterien zum Zurücksetzen der Simulation zu prüfen.
Dabei gilt es die Simulation zurückzusetzen, sobald das Abfangen der angreifenden Drohne erfolgt ist, oder eine der Drohnen in Kontakt mit dem Untergrund kommt.

\subsection{Anforderungen des Laborexperiments}

Anhand der Implementierung des Laborexperiments sollen die notwendigen Messdaten erhoben, die Forschungshypothesen evaluiert, und die Forschungsfrage beantwortet werden.
Dazu ist es notwendig die beschriebenen Metriken, also die Belohnung, dessen Varianz und die Anzahl an Abstürzen, zu allen Testszenarien zu erfassen.
Die Testszenarien werden durch eine Instanz der Simulationsumgebung repräsentiert, die eine veränderte Situation als während des Trainings beinhaltet.

\textbf{Konkrete Beschreibung der Eigenschaften der Testszenarios}

Innerhalb eines Testszenarios sind zu jedem Zeitschritt die aktuelle Belohnung zu speichern und zu Messen, ob ein Absturz der Drohne vorliegt.
Beide Variablen folgen dem Format einer Zeitreihe über den Verlauf der Testsimulation.
Das Programm zur Auswertung muss nach 30-facher Ausführung der Policy im Testszenario die Kennzahlen der durchschnittlichen Belohnung und dessen Varianz berechnen.
Zur Dokumentation und für die weitere Auswertung mittels statistischer Tests sind die Messdaten als kommaseperierte Wertedatei (CSV) zu speichern.

Durch das Laden der Messdaten aus den CSV-Dateien in entsprechende Datenobjekte liegen alle Metriken zur Bestimmung des statistischen Tests vor.
Anschließend sind die Daten durch das Auswertungsprogramm auf ihre Verteilung hin zu untersuchen.
Hierfür ist ein Kolmogorov- oder Shapiro- Test, anhand verfügbarer Softwarebibliotheken zu verwenden.
Können die Messdaten als normalverteilt angenommen werden, gilt es ein T-Test, andernfalls einen Mann-Whitney U Test zur Überprüfung der Hypothesen einzusetzen.

Die genaue Implementierung dieser Tests soll ebenfalls durch zusätzliche Softwarebibliotheken bestimmt sein.
Anhand der Testergebnisse sind durch das Programm die H0 und H1 Hypothesen zu evaluieren und darauf aufbauend die Forschungshypothesen zu bestätigen oder zu verwerfen.
Zusätzlich zu den Messdaten der Testszenarien sind auch die Ergebnisse der Signifikanztests zu speichern.
Schlussendlich sind in einer Tabelle zu jeder Forschungshypothese die Ergebnisse der statistischen Tests anzuführen.
Durch die gesamteinheitliche Betrachtung der Robustheitsmerkmale kann final die Forschungsfrage beantwortet werden.

\section{Implementierung der Simulation und des Experiments}
\subsection{Programmumsetzung der Simulationsumgebung}
\begin{itemize}
    \item verwendetes Repository
    \item dessen Struktur in Form von Aviaries
    \item die Entwicklung einer eigenen Aviary 
    \item Wrapping des eigenen Aviaries zur SingleDrone Applikation
\end{itemize}
\subsection{Programmumsetzung zur Laborexperiments}
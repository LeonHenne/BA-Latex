Anschließend an die Diskussion des aktuellen Stands der Forschung und der Praxis wird im Rahmen dieses Kapitels die Forschungsmethodik angewandt, um die folgende Forschungsfrage zu beantworten:

\textit{Inwiefern kann durch den Einsatz eines mittels RL trainierten Gegenspielers die Robustheit der gelernten Strategie verbessert werden?}

Die in der Forschungsfrage formulierte These untersucht eine mögliche Verbesserung des Sim-to-Real Problems durch trainiertem Gegenspieler.
Für diese Untersuchung wird ein Laborexperiment nach \cite[]{Recker.2021} durchgeführt, indem die Robustheit der erlernten Strategien unter der Verwendung eines deterministischen und eines trainierten Gegenspielers betrachtet wird. 
Die quantitative Auswertung mittels des Laborexperiments kann hierbei besonders die Ursache-Wirkung Beziehung zwischen dem gewählten Trainingsszenarien und der erzielten Robustheit untersuchen.

In der ersten Sektion dieses Kapitels wird auf den Aufbau des Laborexperiments eingegangen.
Dabei werden grundlegende Trainingsdaten sowie zu untersuchende Hypothesen, ausgeübte Einflüsse und erfasste Metriken beschrieben.
Anschließend werden daraus in der nächsten Sektion resultierende Anforderungen für die Entwicklung der Simulation und der Testumgebung abgeleitet. 
Die dritte Sektion setzt daraufhin die beschriebenen Anforderungen um und erläutert die endgültige Implementierung der Simulation, der Testdatenerhebung und deren Auswertung.

\section{Erläuterung der Forschungsmethodik}

Innerhalb des Laborexperiments soll die Beziehung zwischen dem Trainingsszenario als Ursache und der Leistungsrobustheit als Wirkung betrachtet werden.
Dabei sind zunächst Strategien durch verstärkendes Lernen unter unterschiedlichen Szenarien zu optimieren. 
Anschließend werden die trainierten Policies in mehreren Testszenarien einer veränderten Simulation ausgeübt und währenddessen verschiedene Metriken der Strategieleistung betrachtet.
Ziel ist es, die Leistungsdiskrepanzen zu betrachten, zwischen den Strategien des Trainings mit deterministischen und mit optimiertem Gegenspieler.
Kein Teil der umzusetzenden Forschungsmethodik ist daher die Anwendung der simulationsbasierten Strategien zur Steuerung von Quadrokoptern in der echten Welt. 
Weiterhin wird nicht die Ergebnisabhängigkeit zu Faktoren untersucht, wie der Wahl des Simulationsframeworks, der Physik-Engine oder des Abstraktionsniveaus.
Die Auswahl jener Aspekte wird unter den Gesichtspunkten der Softwarearchitektur getroffen, dessen Anforderungen und Implementierung in nachfolgenden Sektionen behandelt wird.

\subsection{Beschreibung der Simulationsumgebung}

Beginnend mit dem Training der Policies durch verstärkendes Lernen, wird hierfür anders als bei überwachtem und unüberwachtem Lernen kein unmittelbar vorliegender Datensatz benötigt.
Anstelle dessen basiert das Training auf der vollständigen oder teilweisen Wahrnehmung einer Lernumgebung, welche den lernenden Agenten für ausgeführte Aktionen positiv oder negativ belohnt.
Wie auch in der Anleitung erwähnt wird im Kontext dieser Arbeit die Simulation von Quadrokoptern dafür verwendet, die Lernumgebung und damit die Trainingsdaten für die Algorithmen des RL zur Verfügung zu stellen.
Die Simulation von Quadrokoptern stellt ein hochdynamisches Anwendungsgebiet dar, bei dem von einer hohen Diskrepanz zwischen der echten und simulierten Welt ausgegangen werden kann. 
Die Simulationsumgebung stellt grundsätzlich ein Szenario dar, in welchem zwei verschiedene Drohnen kompetitiv gegeneinander agieren.
Das Ziel einer Drohne ist es zu einem festgelegten Punkt hinter der zweiten Drohne zu gelangen, währenddessen die zweite Drohne versucht die angreifende Drohne auf seinem Weg abzufangen. 
Die verteidigende Drohne verwendet dafür das Mittel einer bewussten Kollision.
Beide Drohnen werden unter der Einschränkung ihrer Nähe zum Zielpunkt zufällig in einem drei dimensionalem Raum initialisiert, welcher nur horizontal einseitig beschränkt wird.
Durch die einseitige Beschränkung des Flugraums wird der natürliche Untergrund dargestellt.
Zu Beginn soll die verteidigenden Drohne näher am Zielpunkt als die angreifende Drohne starten, sodass stets ein Abfangen möglich ist.
Ist das Ziel einer der beiden Drohnen erreicht, oder besteht Kontakt mit dem Untergrund, so wird die Simulation beendet.

\subsection{Erläuterung der Trainingsszenarien}

Durch die erläuterte Simulation werden insgesamt für das Laborexperiment Strategien in vier unterschiedlichen Szenarien mit verstärkendem Lernen optimiert.
Zur Prüfung der Zuverlässigkeit werden je Trainingsszenario zwei optimale Policies durch zwei verschiedene RL-Algorithmen erlernt.
Jede der Policies wird dabei in einer der nachfolgenden Simulationsszenarien optimiert, dessen Auswahl die unabhängige Variable des Laborexperiments darstellt.
\begin{enumerate}
    \item Das erste Szenario beinhaltet das Training der zu verteidigenden Drohne gegen eine angreifende Drohne, welche eine deterministische regelbasierte Strategie ausführt.
    \item Im zweiten Szenario wird eine deterministische Strategie für die anzugreifende Drohne mittels RL optimiert, während die verteidigende Drohne anhand ihrer zuvor optimierten Strategie agiert.
    \item Anschließend enthält das dritte Szenario das Training einer Verteidigungsstrategie im kompetitiven Spiel mit der anzugreifenden Drohne unter zuvor optimierter Policy.
    \item Zum Abschluss wird das erste Szenario mit regelbasiertem Gegenspieler unter DR, also unter der Randomisierung dynamischer Parameter, als viertes Szenario wiederholt. 
\end{enumerate}

\subsection{Erläuterung der Testszenarien}

Nach den Trainingsphasen werden einzelne Policies in mehreren vom Training abweichenden Testszenarien ausgeführt und deren Leistungsverhalten gemessen.
\textbf{Konkrete Beschreibung der Eigenschaften der Testszenarios}

\subsection{Messung der Robustheit von RL Policies}

Die in den Testszenarien betrachteten abhängigen Variablen sind die durchschnittlich erzielte Belohnung, dessen Varianz sowie die Anzahl an unbeabsichtigten Abstürzen.
Die erzielte Belohnung und dessen Varianz stellen wie bereits beschrieben in der Forschung wichtige Kenngrößen dar, um die Robustheit von RL Policies zu bestimmen.
Die Metrik der Anzahl von unbeabsichtigten Abstürzen spiegelt im behandelten Anwendungsfall zusätzlich das Fehlschlagen der trainierten Strategie wider.
Mit der Auswahl der Metriken werden die Strategieeigenschaften der maximalen Strategieleistung, der Leistungsabweichung und der Strategiesicherheit deutlich.
Aus der Abweichung zwischen dem Leistungsverhalten während des Trainings und während des Tests, soll so die Robustheit von RL Policies erkennbar und messbar gestaltet werden.
Der Fokus liegt dabei auf der Messung der Robustheit von den optimierten Strategien aus Szenario eins, zwei und vier.
Aus dem Vergleich der Robustheit zwischen Strategie eins und zwei kann eine mögliche Verbesserung abgeleitet werden.
Anschließend kann der erzielte Effekt mit der Abweichung zwischen eins und vier verglichen, und so aktuell verwendete Methoden zur Erhöhung der Robustheit einbezogen werden.

\subsection{Auswertung mittels statistischer Tests}

Durch den Vergleich einzelner Strategien mittels dieser Methodik werden im Laborexperiment die zu Beginn der Arbeit aufgestellten Hypothesen auf ihre Gültigkeit untersucht.
Die nachfolgend angeführten Hypothesen beinhalten wie im vorherigen Kapitel diskutiert, verschiedene Aspekte der Robustheit von RL Strategien, welche in späteren Abschnitten einzeln ausgewertet werden.
\begin{enumerate}
    % verschiedene Aspekte: max. Reward, Stabilität des Rewards, Fehlschlag von Strategien
    \item \textit{Die in den Testszenarien durchschnittlich erzielte Belohnung ist unter Verwendung der Policy aus dem Training mit RL basiertem Gegenspieler signifikant und zuverlässig höher als die Policy aus dem Training mit regelbasiertem Gegenspieler.}
    \item \textit{Die Varianz der in den Testszenarien erzielten Belohnung ist unter Verwendung der Policy aus dem Training mit RL basiertem Gegenspieler signifikant und zuverlässig geringer als die Policy aus dem Training mit regelbasiertem Gegenspieler.}
    \item \textit{Die in den Testszenarien erreichte Anzahl von unbeabsichtigten Abstürzen ist unter Verwendung der Policy aus dem Training mit RL basiertem Gegenspieler signifikant und zuverlässig geringer als die Policy aus dem Training mit regelbasiertem Gegenspieler.}
\end{enumerate}

Jede der drei oben genannten Behauptungen wird in zwei statistischen Tests ausgewertet.
Dabei wird zum einen ein Test auf Ungleichheit und zum anderen ein Test zum Verbesserungseffekt durch RL basierten Gegenspieler durchgeführt.
Die Thesen zur ungleichen oder besseren Metrik durch trainierten Gegenspieler werden in den Signifikanztests als H0 Hypothese eingesetzt.
Die entsprechenden Gegenhypothesen formulieren jeweils die gegensätzliche Annahme mit regelbasiertem Gegenspieler. 
Fasst man dieses Aufbau der statistischen Signifikanztests zusammen können folgende Testhypothesen festgelegt werden. 

\textbf{Ungleichheit des ersten Messwerts}
\begin{itemize}
    \item \textbf{Hypothese H0:} \textit{Die durchschnittlich erzielte Belohnung im dritten Testszenarios ist signifikant unterschiedlich zu der des ersten Testszenarios.}
    \item \textbf{Hypothese H1:} \textit{Die durchschnittlich erzielte Belohnung im dritten Testszenarios ist signifikant gleich zu der des ersten Testszenarios.}
\end{itemize}

\textbf{Verbesserung des ersten Messwerts}
\begin{itemize}
    \item \textbf{Hypothese H0:} \textit{Die durchschnittlich erzielte Belohnung im dritten Testszenarios ist signifikant höher zu der des ersten Testszenarios.}
    \item \textbf{Hypothese H1:} \textit{Die durchschnittlich erzielte Belohnung im dritten Testszenarios ist signifikant geringer zu der des ersten Testszenarios.}
\end{itemize}

\textbf{Ungleichheit des zweiten Messwerts}
\begin{itemize}
    \item \textbf{Hypothese H0:} \textit{Die Varianz der erzielten Belohnung im dritten Testszenarios ist signifikant unterschiedlich zu der des ersten Testszenarios.}
    \item \textbf{Hypothese H1:} \textit{Die Varianz der erzielten Belohnung im dritten Testszenarios ist signifikant gleich zu der des ersten Testszenarios.}
\end{itemize}

\textbf{Verbesserung des zweiten Messwerts}
\begin{itemize}
    \item \textbf{Hypothese H0:} \textit{Die Varianz der erzielten Belohnung im dritten Testszenarios ist signifikant geringer zu der des ersten Testszenarios.}
    \item \textbf{Hypothese H1:} \textit{Die Varianz der erzielten Belohnung im dritten Testszenarios ist signifikant höher zu der des ersten Testszenarios.}
\end{itemize}

\textbf{Ungleichheit des dritten Messwerts}
\begin{itemize}
    \item \textbf{Hypothese H0:} \textit{Die erreichte Anzahl von unbeabsichtigten Abstürzen ist im dritten Testszenarios ist signifikant unterschiedlich zu der des ersten Testszenarios.}
    \item \textbf{Hypothese H1:} \textit{Die erreichte Anzahl von unbeabsichtigten Abstürzen ist im dritten Testszenarios ist signifikant gleich zu der des ersten Testszenarios.}
\end{itemize}

\textbf{Verbesserung des dritten Messwerts}
\begin{itemize}
    \item \textbf{Hypothese H0:} \textit{Die erreichte Anzahl von unbeabsichtigten Abstürzen ist im dritten Testszenarios ist signifikant geringer zu der des ersten Testszenarios.}
    \item \textbf{Hypothese H1:} \textit{Die erreichte Anzahl von unbeabsichtigten Abstürzen ist im dritten Testszenarios ist signifikant höher zu der des ersten Testszenarios.}
\end{itemize}

Die Auswahl des Signifikanztests wird anhand der Wahrscheinlichkeit einer vorliegenden Normalverteilung vorgenommen.
Kann nach einem Kolmogorov- oder Shapiro- Test zu einem Signifikanzniveau von 10\% eine Normalverteilung angenommen werden, lässt sich zut Überprüfung der Hypothesen ein T-Test einsetzten.
Ist keine Normalverteilung gegeben, wird ein Mann-Whitney U Test verwendet.
Liegt schlussendlich der P-Wert des T- oder Mann-Whitney U Signifikanztests unter 10\% wird die H0 Hypothese angenommen und die Abweichung in den Robustheitsdaten oder deren Verbesserung als signifikant betrachtet.
Zeigen die statistischen Signifikanztests, dass eine Metrik sich durch die Auswahl des Trainingsszenarios verbessert, wird die Abweichung als signifikant wahrgenommen, und die Hypothese bestätigt.
Die Beantwortung der Forschungsfrage erfolgt abschließend mit der Betrachtung der angenommenen oder abgelehnten Forschungshypothesen.
So werden die verschiedenen Merkmale der RL Policies untersucht und es kann festgestellt werden, welche Effekte auf Robustheit erzielt worden sind.

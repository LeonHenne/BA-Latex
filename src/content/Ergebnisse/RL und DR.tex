\section{Vergleich der Robustheit der Strategien aus dem Training mit RL basiertem Gegenspieler und Domain Randomization}

\begin{table}[htb]
    \begin{tabular}{llllll}
    Metrik & Gleichheit P-Wert & Verschlechterung P-Wert & Ungleichheit & Verbesserung \\
    sum reward & 0.21860488490723795 & 0.10930244245361898 & False & False \\
    \begin{tabular}[c]{@{}l@{}}reward mean \\ difference\end{tabular} & 1.1343177758713004e-13 & 0.9999999999999443 & True & False \\
    \begin{tabular}[c]{@{}l@{}}count of \\ failures \end{tabular} & 0.09899034302740754 & 0.9513450789074275 & True & False
    \end{tabular}
    \caption{Ergebnisse der statistischen Tests aus dem Leistungsvergleich der Modelle aus den Trainingsszenarien 3 und 4}
    \label{tab:my-comparison-3}
\end{table}

Mit Hilfe der Tabelle 9 lassen sich die Ergebniswerte der statistischen Signifikanztests zum Vergleich der Strategien aus RL basiertem Gegenspieler und DR im Training darstellen.
Dazu sind die P-Werte der Tests und Wahrheitswerte der Testhypothesen je untersuchter Metrik angegeben.

\subsection{Betrachtung der kumulierten Belohnung}

\textbf{Hypothese 1:}
\textit{Die im Testszenario erzielte kumulierte Belohnung ist unter Verwendung der Policy aus dem Training mit RL basiertem Gegenspieler signifikant und zuverlässig höher als die Policy aus dem Training mit randomisierter Umgebung.}

% Untersuchungsgegenstand
Zuerst wird die Hypothese 1 einer signifikant höheren kumulierten Belohnung, durch den Einsatz von RL zum Optimieren eines Gegenspielers gegenüber der angewendeten Form von DR, untersucht.
Die H0 Hypothese im ersten Test repräsentiert die Behauptung, dass eine die Daten der Belohnungen einer gemeinsamen Verteilung entstammen. 
Das die Belohnungsdaten, welche durch die Strategie mit RL basiertem Gegenspieler erzeugt worden sind, einer höheren Verteilung entstammen wird im zweiten Test als H0 Hypothese überprüft.
Entsprechend gegensätzliche Annahmen werden in beiden Signifikanztests als H1 Hypothese verwendet.

% Evaluation
Der erste P-Wert der Tabelle 9 in der Zeile zur kumulierten Belohnung liegt mit in etwa $0.2186$ über dem Signifikanzniveau von 10 \%.
Aus diesem Grund wird die H0 Hypothese bestätigt, dass die Daten einer einheitlichen Wahrscheinlichkeitsverteilung entstammen.
Entsprechend ist die H1 Hypothese abzulehnen und auch die obige adaptierte Forschungshypothese zu verwerfen.
Daraus entfällt eine weitere Betrachtung des P-Wertes des zweiten Tests.

% Bezug zur Robustheit
Aus der Überprüfung der Testergebnisse wird deutlich, dass RL keinen verbessernden Effekt auf die Belohnung ausüben konnte als die angewendete Form der DR.
Daraus kann ableitet werden, dass die Strategie aus dem Training mit RL basiertem Gegenspieler nicht robuster agieren konnte, indem eine bessere Belohnung im abweichenden Testszenario erzielt wird.

\subsection{Betrachtung der Belohnungsstabilität}

\textbf{Hypothese 2:}
\textit{Die Varianz der im Testszenario erzielten kumulierten Belohnung ist unter Verwendung der Policy aus dem Training mit RL basiertem Gegenspieler signifikant und zuverlässig geringer als die Policy aus dem Training mit randomisierter Umgebung.}

% Untersuchungsgegenstand
Mittels der nächsten Forschungshypothese soll die Leistungsstabilität, der aus dem Training mit RL basiertem Gegenspieler und mit DR entstandenen Strategien, verglichen werden.
Zur Untersuchung dieser Metrik werden die Daten der Abweichungen der Strategieergebnisse betrachtet.
Als erste H0 Hypothese wird die Annahme einer einheitlichen, als zweite H0 Hypothese die Annahme einer größeren Verteilung eingesetzt.

% Evaluation
Aus Tabelle 9 geht ein P-Wert von in etwa $1.1343e-13$ hervor.
Folgend daraus ist die H0 Hypothese abzulehnen und festzuhalten, dass ein Einfluss des Trainingsszenarios auf die Verteilung der Abweichungen besteht.
Wird dieser Einfluss mittels zweitem Test genauer untersucht, ist dabei der P-Wert von in etwa $1.0$ zu betrachten.
Daraus geht hervor, dass die Wahl eines RL basierten Gegenspieler anstelle der DR im Trainingsszenario einen negativen Effekt auf die Belohnungsstabilität ausübt.

% Bezug zur Robustheit
Entsprechend wird die Forschungshypothese 2 in diesem Kontext verworfen, da kein Verbesserungseffekt durch einen RL basierten Gegenspieler festzustellen war.
Im Gegenteil konnte gezeigt werden, dass ein RL basierter Gegenspieler im Training zu einer weniger robusten Strategie führt, als wenn DR eingesetzt würde.

\subsection{Betrachtung der Anzahl der Misserfolge}

\textbf{Hypothese 3:}
\textit{Die im Testszenario erreichte Anzahl von Misserfolgen ist unter Verwendung der Policy aus dem Training mit RL basiertem Gegenspieler signifikant und zuverlässig geringer als die Policy aus dem Training mit randomisierter Umgebung.}

% Untersuchungsgegenstand
Anhand einer dritten Forschungshypothese soll schlussendlich die dritte Metrik der Robustheit und dessen Einfluss durch das gewählte Trainingsszenario erforscht werden.
Wie in der bisherigen Vorgehensweise wird zunächst die Ähnlichkeit der beiden Datenverteilungen zum Misserfolg der Testepisoden analysiert.
Anschließend ist bei einer vorliegenden Ungleichheit der Effekt des RL basierten Gegenspielers gegenüber der DR herauszustellen.
Zeigt sich aus dieser Gegenüberstellung, dass eine Verbesserung durch das Training mittels RL optimiertem Gegenspieler erzielt wurde, ist die dritte Forschungshypothese anzunehmen. 

% Evaluation
Werden die Testwerte aus Tabelle 9 betrachtet, gilt es zunächst den ersten P-Wert von in etwa $0.0990$ mit dem Signifikanzniveau zu vergleichen.
Die Ablehnung der H0 Hypothese und Annahme einer unterschiedlichen grundlegenden Wahrscheinlichkeitsverteilung folgt aus dem geringer als $0.1$ bemessenen P-Wert.
In der Auswertung des zweiten Tests wird die H0 Hypothese angenommen, da dessen P-Wert mit in etwa $0.9513$ über dem Signifikanzniveau liegt.
Daraus hervorgehend ist festzustellen, dass die Gültigkeit der H1 Hypothese und dementsprechend ein verringernder Effekt auf die Häufigkeit des Misserfolgs durch RL optimierten Gegenspielers auszuschließen ist.

% Bezug zur Robustheit
Mit der Ablehnung der dritten Forschungshypothese lässt sich ein positiverer Effekt des Einsatzes von DR auf die Robustheit festhalten, als mittels des Einsatzes eines RL basierten Gegenspielers.
Der positivere Effekt durch DR wirkt sich dabei im Sinne einer höheren Erfolgsrate auf die Robustheit aus.
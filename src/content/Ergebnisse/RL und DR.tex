\section{Vergleich der Robustheit der Strategien aus dem Training mit RL basiertem Gegenspieler und Domain Randomization}

\begin{table}[htb]
    \begin{tabular}{llllll}
    Metrik & Gleichheit P-Wert & Verschlechterung P-Wert & Ungleichheit & Verbesserung \\
    sum reward & 0.21860488490723795 & 0.10930244245361898 & False & False \\
    \begin{tabular}[c]{@{}l@{}}reward mean \\ difference\end{tabular} & 1.1343177758713004e-13 & 0.9999999999999443 & True & False \\
    \begin{tabular}[c]{@{}l@{}}count of \\ failures \end{tabular} & 0.005412977104604817 & 0.09899034302740754 & 0.9513450789074275 & True & False
    \end{tabular}
    \caption{Ergebnisse der statistischen Tests aus dem Leistungsvergleich der Modelle aus den Trainingsszenarien 3 und 4}
    \label{tab:my-comparison-3}
\end{table}

Mit Hilfe der Tabelle 9 lassen sich die Ergebniswerte der statistischen Signifikanztests zum Vergleich der Strategien aus RL basiertem Gegenspieler und DR im Training darstellen.
Dazu sind die P-Werte der Tests und Wahrheitswerte der Testhypothesen je untersuchter Metrik angegeben.

\subsection{Betrachtung der kumulierten Belohnung}

\textbf{Hypothese 1:}
\textit{Die im Testszenario erzielte kumulierte Belohnung ist unter Verwendung der Policy aus dem Training mit RL basiertem Gegenspieler signifikant und zuverlässig höher als die Policy aus dem Training mit randomisierter Umgebung.}

% Untersuchungsgegenstand
Zuerst wird die Hypothese 1 einer signifikant höheren kumulierten Belohnung, durch den Einsatz von RL zum optimieren eines Gegenspielers gegenüber der angewendeten Form von DR, untersucht.
Die H0 Hypothese im ersten Test repräsentiert die Behauptung das eine die Daten der Belohnungen einer gemeinsamen Verteilung entstammen. 
Das die Belohnungsdaten welche durch die Strategie mit RL basiertem Gegenspieler erzeugt worden sind einer höheren Verteilung entstammen wird im zweiten Test als H0 Hypothese überprüft.
Entsprechend gegensätzliche Annahmen werden in beiden Signifikanztests als H1 Hypothese verwendet.

% Evaluation
Der erste P-wert der Tabelle 9 in der Zeile zur kumulierten Belohnung liegt mit in etwa $0.2186$ über dem Signifikanzniveau von 10\%.
Aus diesem Grund wird die H0 Hypothese bestätigt, dass die Daten einer einheitlichen Wahrscheinlichkeitsverteilung entstammen.
Entsprechend ist die H1 Hypothese abzulehnen und auch die obige adaptierte Forschungshypothese zu verwerfen.
Daraus entfällt eine weitere Betrachtung des P-Wertes des zweiten Tests.

% Bezug zur Robustheit
Aus der Überprüfung der Testergebnisse wird deutlich, dass RL keinen verbessernden Effekt auf die Belohnung ausüben konnte als die angewendete Form der DR.
Daraus kann ableitet werden, dass die Strategie aus dem Training mit RL basiertem Gegenspieler nicht robuster agieren konnte, indem eine bessere Belohnung im abweichenden Testszenario erzielt wird.

\subsection{Betrachtung der Belohnungsstabilität}

\textbf{Hypothese 2:}
\textit{Die Varianz der im Testszenario erzielten kumulierten Belohnung ist unter Verwendung der Policy aus dem Training mit RL basiertem Gegenspieler signifikant und zuverlässig geringer als die Policy aus dem Training mit randomisierter Umgebung.}

% Untersuchungsgegenstand
Mittels der nächsten Forschungshypothese soll die Leistungsstabilität, der aus dem Training mit RL basiertem Gegenspieler und mit DR entstandenen Strategien, verglichen werden.
Zur Untersuchung dieser Metrik werden die Daten der Abweichungen der Strategieergebnisse betrachtet.
Als erste H0 Hypothese wird die Annahme einer einheitlichen, als zweite H0 Hypothese die Annahme einer größeren Verteilung eingesetzt.

% Evaluation
Aus Tabelle 9 geht ein P-Wert von in etwa $1.1343e-13$ hervor.
Folgend daraus ist die H0 Hypothese abzulehnen und festzuhalten, dass ein Einfluss des Trainingsszenarios auf die Verteilung der Abweichungen besteht.
Wird dieser Einfluss mittels zweitem Test genauer untersucht, ist dabei der P-Wert von in etwa $1.0$ zu betrachten.
Daraus geht hervor, dass die Wahl eines RL basierten Gegenspieler anstelle der DR im Trainingsszenario einen negativen Effekt auf die Belohnungsstabilität ausübt.

% Bezug zur Robustheit
Schlussfolgernd konnte damit festgestellt werden, dass ein RL basierter Gegenspieler im Training zu einer weniger robusten Strategie führt, als wenn DR eingesetzt würde.

\textbf{Hier am Ende vlt auch die wirkliche Varianz berechnen}

\subsection{Betrachtung der Anzahl der Misserfolge}

\textbf{Hypothese 3:}
\textit{Die im Testszenario erreichte Anzahl von Misserfolgen ist unter Verwendung der Policy aus dem Training mit RL basiertem Gegenspieler signifikant und zuverlässig geringer als die Policy aus dem Training mit randomisierter Umgebung.}

% Untersuchungsgegenstand


% Evaluation


% Bezug zur Robustheit

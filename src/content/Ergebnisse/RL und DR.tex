\section{Vergleich der Robustheit der Strategien aus dem Training mit regelbasierten Gegenspieler und Domain Randomization}

\begin{table}[htb]
    \begin{tabular}{llllll}
    Metrik & Gleichheit P-Wert & Verschlechterung P-Wert & Ungleichheit & Verbesserung \\
    sum reward & 0.21860488490723795 & 0.10930244245361898 & False & False \\
    \begin{tabular}[c]{@{}l@{}}reward mean \\ difference\end{tabular} & 1.1343177758713004e-13 & 0.9999999999999443 & True & False \\
    \begin{tabular}[c]{@{}l@{}}count of \\ failures \end{tabular} & 0.005412977104604817 & 0.09899034302740754 & 0.9513450789074275 & True & False
    \end{tabular}
    \caption{Ergebnisse der statistischen Tests aus dem Leistungsvergleich der Modelle aus den Trainingsszenarien 3 und 4}
    \label{tab:my-comparison-3}
\end{table}

\subsection{Betrachtung der kumulierten Belohnung}

\textbf{Hypothese 1:}
\textit{Die im Testszenario erzielte kumulierte Belohnung ist unter Verwendung der Policy aus dem Training mit RL basiertem Gegenspieler signifikant und zuverlässig höher als die Policy aus dem Training mit regelbasiertem Gegenspieler.}

\subsection{Betrachtung der Belohnungsstabilität}

\textbf{Hypothese 2:}
\textit{Die Varianz der im Testszenario erzielten kumulierten Belohnung ist unter Verwendung der Policy aus dem Training mit RL basiertem Gegenspieler signifikant und zuverlässig geringer als die Policy aus dem Training mit regelbasiertem Gegenspieler.}

\textbf{Hier am Ende vlt auch die wirkliche Varianz berechnen}

\subsection{Betrachtung der Anzahl der Misserfolge}

\textbf{Hypothese 3:}
\textit{Die im Testszenario erreichte Anzahl von Misserfolgen ist unter Verwendung der Policy aus dem Training mit RL basiertem Gegenspieler signifikant und zuverlässig geringer als die Policy aus dem Training mit regelbasiertem Gegenspieler.}
Anschließend an die Durchführung des Laborexperiments werden innerhalb dieses Kapitels die erzeugten Messdaten betrachtet und die zur Beantwortung der Forschungsfrage aufgestellten Hypothesen ausgewertet.
Es gilt dabei einen möglichen Effekt des trainierten Gegenspielers auf die Robustheit des RL-Modells zu untersuchen.
Dazu werden die Leistungen der Strategien aus den unterschiedlichen Trainingsszenarien im abweichenden Testszenario verglichen.
Eine Leistungsmessung im abweichenden Testszenario zeigt, welche Lernumgebung eine stabilere und robustere Strategie hervorbringt, welche auch unter unbekannten Effekten performanter ist.

Die auszuwertenden Leistungsmetriken aggregiert je Testepisode können im Anhang gefunden werden.
Im ersten Vergleich werden die Strategien aus dem Training mit regelbasiertem und mittels RL optimierten Gegenspieler untersucht.
Weiterhin wird ein Vergleich der Testmetriken zwischen dem Training ohne und mit randomisierter Umgebung bei regelbasiertem Gegenspieler durchgeführt.
Dazu werden mehrfach dieselben statistischen Signifikanztests durchgeführt, um zum einen den Effekt der DR zu ermitteln.
Zum anderen werden die Einflüsse der im Rahmen dieser Arbeit entwickelten Methoden wie der DR und dem RL basierten Gegenspieler gegenübergestellt.
Das Ende dieses Kapitels bildet die Beantwortung der Forschungsfrage anhand der ausgewerteten Test- und Forschungshypothesen.

\section{Vergleich der Robustheit der Strategien aus dem Training mit regelbasierten und RL Gegenspieler}

\begin{table}[htb]
    \begin{tabular}{llllll}
    Metrik & Gleichheit P-Wert & Verschlechterung P-Wert & Ungleichheit & Verbesserung \\
    \begin{tabular}[c]{@{}l@{}}Erzielte kumu- \\lierte Belohnung\end{tabular} & 0.6276673409642575 & 0.6870324210336459 & False & False \\
    \begin{tabular}[c]{@{}l@{}}Abweichung vom \\ Belohnungs- \\Mittelwert\end{tabular} & 1.2200477617083057e-22 & 1.0 & True & False \\
    \begin{tabular}[c]{@{}l@{}}Häufigkeit von \\ Misserfolg \end{tabular} & 0.1814596593937995 & 0.09072982969689974 & False & True
    \end{tabular}
    \caption{Ergebnisse der statistischen Tests aus dem Leistungsvergleich der Modelle aus den Trainingsszenarien 1 und 3}
    \label{tab:my-table}
\end{table}

Anhand Tabelle 7 lassen sich die Ergebnisse der statistischen Signifikanztests ablesen.
Die Ergebnisse basieren dabei auf dem Leistungsvergleich der Modelle aus den Trainingsszenarien 1 und 3.
Den Tabellenspalten werden zum einen die P-Werte der Tests zur Gleichheit und Verschlechterung der Verteilung zugeordnet.
Zum anderen wird darauf basierend die Erfüllung der H1-Hypothese anhand des gewählten Signifikanzniveaus von 10 \% evaluiert.
Einer Zeile der Tabelle können die Ergebnisdaten zu einer der drei untersuchten Metriken entnommen werden. % Den Zeilen der Tabelle ... zu je einer der drei 

\subsection{Betrachtung der kumulierten Belohnung}

\textbf{Hypothese 1:}
\textit{Die im Testszenario erzielte kumulierte Belohnung ist unter Verwendung der Policy aus dem Training mit RL basiertem Gegenspieler signifikant und zuverlässig höher als die Policy aus dem Training mit regelbasiertem Gegenspieler.}

% Untersuchungsgegenstand
Die erste in der Einleitung aufgestellte Forschungshypothese untersucht den Trainingseinfluss auf die in einer Testepisode erzielte kumulierte Belohnung.
Mit der Hypothese 1 wird die Behauptung aufgestellt, dass durch die Wahl eines Trainingsszenarios mit RL-optimierten Gegenspieler zuverlässig eine bessere kumulierte Belohnung erzielt werden kann, als mittels eines Trainings mit regelbasierten Gegenspieler.
Die Auswertung der Hypothese erfolgt durch statistische Signifikanztests zur Verteilung der kumulierten Belohnungen je Testepisode.
Zur Bestätigung dieser Forschungshypothese müssen die P-Werte zur Gleichheit und Verbesserung der Belohnungsverteilung jeweils geringer als das Signifikanzniveau sein. 

% Evaluation
Der P-Wert des Signifikanztests zur Gleichheit der Verteilungen der kumulierten Belohnungen liegt in etwa bei $0.6277$.
Daraus kann festgestellt werden, dass dieser größer als das gewählte Signifikanzniveau von 10 \% bzw. $0.1$ ist.
Die im Test untersuchte H0 Hypothese einer gleichen unterliegenden Verteilung ist damit anzunehmen.
Eine Ungleichheit der unterliegenden Verteilung der Belohnungen, wie in Hypothese H1 behauptet, ist folglich abzulehnen.
Da demnach keine Unterschiede in der Verteilung der im Testszenario erzielten Belohnungen vorliegen, ist festzuhalten, dass die Forschungshypothese 1 abgelehnt wird.
% Somit ist festzuhalten, dass die Forschungshypothese 1 aufgrund der nicht vorliegenden Unterschiede in der Verteilung der im Testszenario erzielten Belohnungen abzulehnen ist.
Im Laborexperiment konnte somit kein Nachweis erbracht werden, dass ein Optimieren eines RL-Modells, unter Verwendung eines selbst mittels RL optimierten Gegenspielers, zuverlässig zu einer höheren kumulierten Belohnung führt.

% Bezug zur Robustheit
Schließt man von diesem Ergebnis auf dessen Bedeutung für die Robustheit der Strategien aus verstärkendem Lernen zeigt sich folgende Erkenntnis.
Betrachtet man lediglich den Aspekt der kumulierten Belohnung einer Episode, kann eine Lernumgebung mit RL-basiertem Gegenspieler keinen positiveren Effekt auf die Leistungsfähigkeit unter unbekanntem Umgebungsverhalten ausüben.
Ebenso ist auch festzuhalten, dass keine Verschlechterung der Leistungsfähigkeit im Testszenario vorliegt. 
Ein Einfluss eines RL-basierten Gegenspielers auf die Robustheit der kumulierten Belohnung unter unbekannten Umgebungsverhalten kann demnach als nicht gegeben betrachtet werden.

\subsection{Betrachtung der Belohnungsstabilität}

\textbf{Hypothese 2:}
\textit{Die Varianz der im Testszenario erzielten kumulierten Belohnung ist unter Verwendung der Policy aus dem Training mit RL basiertem Gegenspieler signifikant und zuverlässig geringer als die Policy aus dem Training mit regelbasiertem Gegenspieler.}

% Untersuchungsgegenstand
Mittels der Betrachtung der zweiten Forschungshypothese wird die Stabilität der im Testszenario erzielten kumulierten Belohnung untersucht.
Dazu wird die Abweichung einer Belohnung aus einer Testepisode zum Mittelwert der Belohnungen über aller 100 Testepisoden berechnet.
Forschungshypothese 2 stellt dabei die Behauptung auf, dass eine geringere Varianz der Belohnungen im Testszenario vorliegen unter Auswahl eines Trainingsszenarios mit RL basiertem Gegenspieler. % unter Auswahl der Strategie aus einem Trainingsszenario mit RL basiertem Gegenspieler
Zur Bestätigung dieser Hypothese gilt es zum einen, eine Ungleichheit der unterliegenden Verteilung der Abweichungen festzustellen.
Zum anderen ist es notwendig, dass die Verteilung der Abweichungen zur Basis des RL basiertem Gegenspielers signifikant geringer ist.

% Evaluation
Zur Evaluation der Forschungshypothese 2 unterstützen die Ergebnisse der Tabelle 7.
Aus der Tabelle wird erkenntlich, dass der P-Wert des Signifikanztests zur Gleichheit bei in etwa $1.2200e-22$ liegt.
Da sich dieser Wert unter dem gewählten Signifikanzniveau befindet, kann daraus abgeleitet werden, dass nur sehr unwahrscheinlich eine Gleichheit der unterliegenden Verteilung vorherrscht.
Die aufgestellte H0 Hypothese wird daher verworfen, und die gegensätzliche H1 Hypothese einer Ungleichheit der beiden Verteilungen wird angenommen.
Wird hinzu der P-Wert des Signifikanztests zur Verschlechterung der Belohnungsstabilität von $1.0$ betrachtet, kann die folgende Erkenntnis festgestellt werden.
Aufgrund des über dem Signifikanzniveau befindlichen P-Wertes, ist die H0 Hypothese zu bestätigen und von einer geringeren Varianz der erzielten Belohnungen auszugehen unter dem Training mit regelbasiertem Gegenspieler.
Eine Verringerung der Belohnungsvarianz durch Optimierung unter RL basierten Gegenspieler, wie in der H1 Hypothese behauptet, ist demnach auszuschließen.
%\textbf{Hier am Ende vlt auch die wirkliche Varianz berechnen}

% Bezug zur Robustheit
Wird diese Erkenntnis im Kontext der Messung der Robustheit betrachtet, werden zwei Effekte deutlich.
Zum einen zeigte sich mit der Durchführung des Laborexperiments, dass die Gestaltung des Gegenspielers in der Trainingsumgebung einen Effekt auf die Belohnungsstabilität ausübt.
Zum Anderen wird deutlich, dass der Einsatz eines optimierten Gegenspielers zu einer Verschlechterung der Belohnungsstabilität führt.
Eine RL-Policy, welche unter einem durch verstärkendes Lernen optimierten Gegenspieler trainiert wurde, erzielt demnach eine geringere Robustheit aus der höheren Varianz der Belohnung. % geringere Robustheit gemessen an der höheren Varianz der Belohnung

\subsection{Betrachtung der Anzahl der Misserfolge}

\textbf{Hypothese 3:}
\textit{Die im Testszenario erreichte Anzahl von Misserfolgen ist unter Verwendung der Policy aus dem Training mit RL basiertem Gegenspieler signifikant und zuverlässig geringer als die Policy aus dem Training mit regelbasiertem Gegenspieler.}

% Untersuchungsgegenstand
Als dritte Forschungshypothese zur Beantwortung der Forschungsfrage wurde die Behauptung aufgestellt, dass die erreichte Anzahl von Misserfolgen im Testszenario signifikant geringer ist durch den Einsatz des RL-basierten Gegenspielers.
Zur Untersuchung dessen wird die Verteilung der erfolgreichen und erfolglosen Testepisoden mittels statistischer Signifikanztests ausgewertet. 
Weisen die Ergebnisse des Tests zur Gleichheit und Verschlechterung der Metrik je einen P-Wert aus, welcher unter dem Signifikanzniveau liegt, so kann die Forschungshypothese bestätigt und von einem positiven Effekt ausgegangen werden.

% Evaluation
Betrachtet man zunächst den P-Wert zur Gleichheit der Verteilungen von erfolgreichen und erfolglosen Episoden, liegt dieser anhand Tabelle 7 bei in etwa $0.1815$. % laut Tabelle 7 / entsprechend Tabelle 7 / nach Tabelle 7
Daraus, dass dieser Wert geringer als das Signifikanzniveau ist, wird die H0 Hypothese einer Gleichheit der Verteilungen angenommen. % Daher, dass
Entsprechend wird die H1 Hypothese einer ungleichen Verteilung der Misserfolge abgelehnt, sodass kein Unterschied in den Erfolgsverteilungen der RL-Policies festzustellen war.
% Zum anderen zeigt Tabelle 7 einen P-wert von in etwa $0.0027$ zur Verschlechterung der Metrik, also einer höheren Verteilung des Misserfolgs bei Einsatz der Strategie aus dem Training mit RL basiertem Gegenspieler.
% Da auch hier ein P-Wert vorliegt, welcher geringer als das Signifikanzniveau ist, wird erneut die H0 Hypothese abgelehnt.
% Die Gegenhypothese H1, welche eine geringere Verteilung des Misserfolgs im Testszenario behauptet, ist daraus hervorgehend anzunehmen.
Mit der Bestätigung keiner unterschiedlichen Verteilung des Misserfolgs bei einem Training mit RL basiertem Gegenspieler ist die Forschungshypothese 3 abzulehnen.

% Bezug zur Robustheit
Das Ablehnen der dritten Forschungshypothese bestätigt, dass kein Effekt durch die Wahl des Trainingsszenarios auf die Robustheit entstanden ist.
Wird lediglich die Metrik des Misserfolgs betrachtet, konnte festgestellt werden, dass unter unbekanntem Umgebungsverhalten ähnlich häufig Misserfolge mittels der Strategie erzielt worden sind, welche den RL basierten Gegenspieler im Trainingsszenario verwendete.
Insgesamt kann dadurch festgehalten werden, dass keine Verbesserung der Robustheit mit der im Rahmen dieser Arbeit angewendeten Methodik erzielt wurde.

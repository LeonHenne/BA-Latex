Anschließend an die Durchführung des Laborexperiments werden innerhalb dieses Kapitels die erzeugten Messdaten betrachtet und die zur Beantwortung der Forschungsfrage aufgestellten Hypothesen ausgewertet.
Es gilt dabei einen möglichen Effekt des trainierten Gegenspielers auf die Robustheit des RL-Modells zu untersuchen.
Dazu werden die Leistungen der Strategien aus den unterschiedlichen Trainingsszenarien im abweichenden Testszenario verglichen.
Eine Leistungsmessung im abweichenden Testszenario zeigt, welche Lernumgebung eine stabilere und robustere Strategie hervorbringt, welche auch unter unbekannten Effekten performanter ist.

Die auszuwertenden Leistungsmetriken aggregiert zu ihrer Testepisode können im Anhang gefunden werden.
Im ersten Vergleich werden die Strategie aus dem Training mit regelbasiertem und mittels RL optimierten Gegenspieler untersucht.
Weiterhin wird ein Vergleich der Testmetriken zwischen dem Training ohne und mit randomisierter Umgebung sowie regelbasiertem Gegenspieler durchgeführt.
Dazu werden mehrfach die selben statistischen Signifikanztests durchgeführt, um zum einen den Effekt der DR zu ermitteln und zum anderen diesen anschließend mit dem des RL basierten Trainings zu vergleichen.
Das Ende dieses Kapitels bildet die Beantwortung der Forschungsfrage anhand der ausgewerteten Test- und Forschungshypothesen.

\section{Vergleich der Robustheit der Strategien aus dem Training mit regelbasierten und RL Gegenspieler}

\begin{table}[htb]
    \begin{tabular}{llllll}
    Metrik & Gleichheit P-Wert & Verschlechterung P-Wert & Ungleichheit & Verbesserung \\
    sum reward & 0.48390799726891964 & 0.7588082897727775 & False & False \\
    \begin{tabular}[c]{@{}l@{}}reward mean \\ difference\end{tabular} & 0.007654101554906155 & 0.9962006818665161 & True & False \\
    \begin{tabular}[c]{@{}l@{}}count of \\ failures \end{tabular} & 0.005412977104604817 & 0.0027064885523024086 & True & True
    \end{tabular}
    \caption{Ergebnisse der statistischen Tests aus dem Leistungsvergleich der Modelle aus den Trainingsszenarien 1 und 3}
    \label{tab:my-table}
\end{table}

Anhand Tabelle 7 lassen sich die Ergebnisse der statistischen Signifikanztests ablesen.
Die Ergebnisse basieren dabei auf dem Leistungsvergleich der Modelle aus den Trainingsszenarien 1 und 3.
Den Tabellenspalten werden zum einen die P-Werte der Tests zur Gleichheit und Verschlechterung der Verteilung zugeordnet.
Zum anderen wird darauf basierend die Erfüllung der H1-Hypothese anhand des gewählten Signifikanzniveaus von 10\% evaluiert.
Einer Zeile der Tabelle können die Ergebnissdaten zu einer der drei untersuchten Metriken entnommen werden.

\subsection{Betrachtung der kumulierten Belohnung}

\textbf{Hypothese 1:}
\textit{Die im Testszenario erzielte kumulierte Belohnung ist unter Verwendung der Policy aus dem Training mit RL basiertem Gegenspieler signifikant und zuverlässig höher als die Policy aus dem Training mit regelbasiertem Gegenspieler.}

% Untersuchungsgegenstand
Die erste in der Einleitung aufgestellte Forschungshypothese untersucht den Trainingseinfluss auf die in einer Testepisode erzielte kumulierte Belohnung.
Mit der Hypothese 1 wird die Behauptung aufgestellt, dass durch die Wahl eines Trainingsszenarios mit RL-optimierten Gegenspieler zuverlässig eine bessere kumulierte Belohnung erzielt werden kann, als mittels eines Trainings mit regelbasierten Gegenspieler.
Die Auswertung der Hypothese erfolgt mittels statistischer Signifikanztests zur Verteilung der kumulierten Belohnungen je Testepisode.
Zur Bestätigung dieser Forschungshypothese sind die P-Werte zur Gleichheit und Verbesserung der Belohnungsverteilung über alle Testepisoden aus Tabelle 7 zu betrachten. 

% Evaluation
Der P-Wert des Signifikanztests zur Gleichheit der Verteilungen der kumulierten Belohnungen liegt in etwa bei $0.4839$.
Daraus kann festgestellt werden, dass dieser größer als das gewählte Signifikanzniveau von 10\% bzw. $0.1$ ist.
Die im Test untersuchte H0 Hypothese einer gleichen unterliegenden Verteilung ist damit anzunehmen.
Eine Ungleichheit der unterliegenden Verteilung der Belohnungen, wie in Hypothese H1 behauptet, ist folglich abzulehnen.
Da demnach keine Unterschiede in der Verteilung der im Testszenario erzielten Belohnungen vorliegen ist festzuhalten, dass die Forschungshypothese 1 abgelehnt wird.
% Somit ist festzuhalten, dass die Forschungshypothese 1 aufgrund der nicht vorliegenden Unterschiede in der Verteilung der im Testszenario erzielten Belohnungen abzulehnen ist.
Im Laborexperiment konnte somit kein Nachweis erbracht werden, dass ein Optimieren eines RL-Modells, unter Verwendung eines selbst mittels RL optimierten Gegenspielers, zuverlässig zu einer höheren kumulierten Belohnung führt.

% Bezug zur Robustheit
Schließt man von diesem Ergebnis auf dessen Bedeutung für die Robustheit der Strategien aus verstärkendem Lernen zeigt sich folgende Erkenntnis.
Betrachtet man lediglich den Aspekt der kumulierten Belohnung einer Episode, kann eine Lernumgebung mit RL-basiertem Gegenspieler keinen positiveren Effekt auf die Leistungsfähigkeit unter unbekanntem Umgebungsverhalten ausüben.
Ebenso ist auch festzuhalten, dass keine Verschlechterung der Leistungsfähigkeit im Testszenario vorliegt. 
Ein Einfluss eines RL-basierten Gegenspielers auf die Robustheit der kumulierten Belohnung unter unbekannten Umgebungsverhalten kann demnach als nicht gegeben betrachtet werden.

\subsection{Betrachtung der Belohnungsstabilität}

\textbf{Hypothese 2:}
\textit{Die Varianz der im Testszenario erzielten kumulierten Belohnung ist unter Verwendung der Policy aus dem Training mit RL basiertem Gegenspieler signifikant und zuverlässig geringer als die Policy aus dem Training mit regelbasiertem Gegenspieler.}

% Untersuchungsgegenstand
Mittels der Betrachtung der zweiten Forschungshypothese wird die Stabilität der im Testszenario erzielten kumulierten Belohnung untersucht.
Dazu wird die Abweichung einer Belohnung aus einer Testepisode zum Mittelwert der Belohnungen über aller 100 Testepisoden berechnet.
Forschungshypothese 2 stellt dabei die Behauptung auf, dass eine geringere Varianz der Belohnungen im Testszenario vorliegen unter Auswahl eines Trainingsszenarios mit RL basiertem Gegenspieler.
Zur Bestätigung dieser Hypothese gilt es zum einen, eine Ungleichheit der unterliegenden Verteilung der Abweichungen festzustellen.
Zum anderen ist es notwendig, dass die Verteilung der Abweichungen zur Basis des RL basiertem Gegenspielers signifikant geringer ist.

% Evaluation
Zur Evaluation der Forschungshypothese 2 unterstützen die Ergebnisse der Tabelle 7.
Aus der Tabelle wird erkenntlich, dass der P-Wert des Signifikanztests zur Gleichheit bei in etwa $0.0077$ liegt.
Da sich dieser Wert unter dem gewählten Signifikanzniveau befindet, kann daraus abgeleitet werden, dass nur sehr unwahrscheinlich eine Gleichheit der unterliegenden Verteilung vorherrscht.
Die aufgestellte H0 Hypothese wird daher verworfen, und die gegensätzliche H1 Hypothese einer Ungleichheit der beiden Verteilungen wird angenommen.
Wird hinzu der P-Wert des Signifikanztests zur Verschlechterung der Belohnungsstabilität von in etwa $0.9962$ betrachtet, kann die folgende Erkenntnis festgestellt werden.
Aufgrund des über dem Signifikanzniveau befindlichen P-Wertes, ist die H0 Hypothese zu bestätigen und von einer geringeren Varianz der erzielten Belohnungen auszugehen unter dem Training mit regelbasiertem Gegenspieler.
Eine Verringerung der Belohnungsvarianz durch Optimierung unter RL basierten Gegenspieler, wie in der H1 Hypothese behauptet, ist demnach auszuschließen.
%\textbf{Hier am Ende vlt auch die wirkliche Varianz berechnen}

% Bezug zur Robustheit
Wird diese Erkenntnis im Kontext der Messung der Robustheit betrachtet werden zwei Effekte deutlich.
Zum Einen zeigte sich mit der Durchführung des Laborexperiments, dass die Gestaltung des Gegenspielers in der Trainingsumgebung einen Effekt auf die Belohnungsstabilität ausübt.
Zum Anderen wird deutlich, dass dieser Effekt lediglich zu einer Verschlechterung der Belohnungsstabilität führt.
Eine RL-Policy welche unter einem Gegenspieler welcher durch verstärkendes Lernen optimiert wurde, trainiert wird erzielt demnach eine geringere Robustheit aus der höheren Varianz der Belohnung.

\subsection{Betrachtung der Anzahl der Misserfolge}

\textbf{Hypothese 3:}
\textit{Die im Testszenario erreichte Anzahl von Misserfolgen ist unter Verwendung der Policy aus dem Training mit RL basiertem Gegenspieler signifikant und zuverlässig geringer als die Policy aus dem Training mit regelbasiertem Gegenspieler.}

% Untersuchungsgegenstand
Als dritte Forschungshypothese zur Beantwortung der Forschungsfrage wurde die Behauptung aufgestellt, dass die erreichte Anzahl von Misserfolgen im Testszenario signifikant geringer ist durch den Einsatz des RL-basierten Gegenspielers.
Zur Untersuchung dessen wird die Verteilung der erfolgreichen und erfolglosen Testepisoden mittels statistischer Signifikanztests ausgewertet. 
Weisen die Ergebnisse des Tests zur Gleichheit und Verschlechterung der Metrik je einen P-Wert aus, welcher unter dem Signifikanzniveau liegt, so kann die Forschungshypothese bestätigt und von einem positiven Effekt ausgegangen werden.

% Evaluation
Betrachtet man die P-Werte der Tabelle 7 zur Metrik der Anzahl an Misserfolgen liegt zum einen der P-Wert zur Gleichheit der Verteilungen beider untersuchten RL-Policies bei in etwa $0.0054$.
Daraus, dass dieser Wert geringer als das Signifikanzniveau ist, wird die H0 Hypothese einer Gleichheit der Verteilungen abgelehnt.
Entsprechend wird die H1 Hypothese einer ungleichen Verteilung der Misserfolge angenommen, sodass ein Unterschied in den Erfolgsverteilungen der RL-Policies festzustellen ist.
Zum anderen zeigt Tabelle 7 einen P-wert von in etwa $0.0027$ zur Verschlechterung der Metrik, also einer höheren Verteilung des Misserfolgs bei Einsatz der Strategie aus dem Training mit RL basiertem Gegenspieler.
Da auch hier ein P-Wert vorliegt, welcher geringer als das Signifikanzniveau ist, wird erneut die H0 Hypothese abgelehnt.
Die Gegenhypothese H1, welche eine geringere Verteilung des Misserfolgs im Testszenario behauptet, ist daraus hervorgehend anzunehmen.
Mit der Bestätigung einer unterschiedlichen und geringen Verteilung des Misserfolgs bei einem Training mit RL basiertem Gegenspieler ist die Forschungshypothese 3 anzunehmen.

% Bezug zur Robustheit
Die Annahme der dritten Forschungshypothese bestätigt, dass ein Effekt durch die Wahl des Trainingsszenarios auf die Robustheit entstanden ist.
Wird lediglich die Metrik des Misserfolgs betrachtet konnte festgestellt werden, dass unter unbekanntem Umgebungsverhalten weniger Misserfolge mittels der Strategie erzielt worden sind, welche den RL basierten Gegenspieler im Trainingszenario verwendete.
Insgesamt konnte dadurch bestätigt werden, dass eine Verbesserung der Robustheit mit der im Rahmen dieser Arbeit angewendeten Methodik erzielt wurde.

\section{Vergleich der Robustheit der Strategien aus dem Training mit regelbasierten Gegenspieler und Domain Randomization}

\begin{table}[htb]
    \begin{tabular}{llllll}
    Metrik & Gleichheit P-Wert & Verschlechterung P-Wert & Ungleichheit & Verbesserung \\
    sum reward & 0.48390799726891964 & 0.7588082897727775 & False & False \\
    \begin{tabular}[c]{@{}l@{}}reward mean \\ difference\end{tabular} & 2.2302103039562726e-05 & 0.9999889697144758 & True & False \\
    \begin{tabular}[c]{@{}l@{}}count of \\ failures \end{tabular} & 0.005412977104604817 & 0.0027064885523024086 & True & True
    \end{tabular}
    \caption{Ergebnisse der statistischen Tests aus dem Leistungsvergleich der Modelle aus den Trainingsszenarien 1 und 4}
    \label{tab:my-comparison-2}
\end{table}

Tabelle 8 weist die Ergebnisse der statistischen Signifikanztests zum Vergleich der Strategieleistungen aus dem Training mit regelbasiertem Gegenspieler und DR aus.
Dabei werden in gleicher Struktur zur Tabelle 7 die P-Werte zur Gleichheit und Verschlechterung der Verteilung sowie die Annahme der H1 Hypothesen anhand jeder Metrik dargestellt.

\subsection{Betrachtung der kumulierten Belohnung}

\textbf{Hypothese 1:}
\textit{Die im Testszenario erzielte kumulierte Belohnung ist unter Verwendung der Policy aus dem Training mit DR signifikant und zuverlässig höher als die Policy aus dem Training ohne Randomisierung der Umgebung.}

% Untersuchungsgegenstand
Für die Ermittlung des Effekts der DR wird die Forschungshypothese 1 für den Vergleich der Strategieleistungen aus dem Training mit und ohne DR adaptiert.
Die Hypothese stellt damit die Behauptung auf, dass die kumulierte Belohnung signifikant höher ist, sofern ein Training mit DR vorliegt.
Dessen Auswertung erfolgt über eine ähnliche Adaptierung der H0 und H1 Hypothesen der Tests zur Gleichheit und Verschlechterung der Ergebnissdaten.
Um die adaptierte Hypothese zu bestätigen gilt es eine Ungleichheit im ersten Signifikanztest und eine Verbesserung im zweiten Test festzustellen.

% Evaluation
Werden zur Auswertung die P-Werte aus der Tabelle 8 betrachtet liegt unter anderem ein P-Wert aus dem Test der Gleichheit von in etwa $0.4839$ vor.
Da dieser Wert über dem Signifikanzniveau liegt, ist die entsprechende Null Hypothese anzunehmen.
Daraus ist festzustellen, dass die H1 Hypothese verworfen wird und demnach von einer gleichen Verteilung der Belohnungen in Abhängigkeit des Trainingsszenarios auszugehen ist.
% Wird der P-Wert des zweiten Signifikanztests herangezogen, so wird erkenntlich, dass eine Verschlechterung der Belohnungsverteilung erzielt worden ist mittels der Strategie, welche unter DR optimiert wurde.
% Dies geht daraus hervor, dass der P-Wert mit in etwa $0.9533$ größer als das Signifikanzniveau ist.
% Im Test zur Verschlechterung der Metrik wird dadurch die H0 Hypothese bestätigt und die H1 Hypothese dem entsprechend abgelehnt.

% Bezug zur Robustheit
Die im Rahmen dieser Arbeit verwendete Form der Domänen Randomisierung zeigte somit keinen Effekt auf die Robustheit der Strategien gegenüber des unbekannten Testszenarios.
% Aus den Ergebnissen der Signifikanztests geht hervor, dass mittels der in dieser Arbeit angewendeten DR im Trainingsprozess ein negativer Effekt auf die Belohnungsverteilung ausgeübt wurde.
Für das beschriebene Testszenario konnte der Einsatz von DR im Trainingsszenario demnach keine Erhöhung der Robustheit, im Sinne einer verbesserten Belohnungsverteilung, hervorgerufen werden.

\subsection{Betrachtung der Belohnungsstabilität}

\textbf{Hypothese 2:}
\textit{Die Varianz der im Testszenario erzielten kumulierten Belohnung ist unter Verwendung der Policy aus dem Training mit DR signifikant und zuverlässig geringer als die Policy aus dem Training ohne Randomisierung der Umgebung.}

% Untersuchungsgegenstand
Mittels der adaptierten zweiten Forschungshypothese wird die Verteilung der Abweichungen 

% Evaluation

% Bezug zur Robustheit
\textbf{Hier am Ende vlt auch die wirkliche Varianz berechnen}

\subsection{Betrachtung der Anzahl der Misserfolge}

\textbf{Hypothese 3:}
\textit{Die im Testszenario erreichte Anzahl von Misser folgen ist unter Verwendung der Policy aus dem Training mit DR signifikant und zuverlässig geringer als die Policy aus dem Training ohne Randomisierung der Umgebung.}

% Untersuchungsgegenstand
% Evaluation
% Bezug zur Robustheit
% Encoding: UTF-8

% This file was created with Citavi 6.12.0.177

@proceedings{.1997,
  year      = {1997},
  title     = {Proceedings of the 29th conference on Winter simulation  - WSC '97},
  address   = {New York, New York, USA},
  publisher = {{ACM Press}}
}

@proceedings{.2014,
  year      = {2014},
  title     = {2014 IEEE International Conference on Robotics and Automation (ICRA)},
  publisher = {IEEE},
  file      = {Reinforcement learning with multi-fidelity simulators:Attachments/Reinforcement learning with multi-fidelity simulators.pdf:application/pdf}
}

@proceedings{.2018,
  year      = {2018},
  title     = {2018 14th International Wireless Communications {\&} Mobile Computing Conference (IWCMC)},
  publisher = {IEEE}
}

@proceedings{.2019,
  year      = {2019},
  title     = {2019 International Conference on Robotics and Automation (ICRA)},
  publisher = {IEEE},
  file      = {A Data-Efficient Framework for Training and Sim-to-Real:Attachments/A Data-Efficient Framework for Training and Sim-to-Real.pdf:application/pdf}
}

@proceedings{.2019,
  year      = {2019},
  title     = {2019 International Conference on Robotics and Automation (ICRA)},
  publisher = {IEEE},
  file      = {A Data-Efficient Framework for Training and Sim-to-Real:Attachments/A Data-Efficient Framework for Training and Sim-to-Real.pdf:application/pdf}
}

@proceedings{.2020,
  year      = {2020},
  title     = {2020 IEEE Symposium Series on Computational Intelligence (SSCI)},
  publisher = {IEEE},
  file      = {Sim-to-Real Transfer in Deep Reinforcement Learning for:Attachments/Sim-to-Real Transfer in Deep Reinforcement Learning for.pdf:application/pdf}
}

@proceedings{.2021,
  year      = {2021},
  title     = {Proceedings of the 22nd International Workshop on Mobile Computing Systems and Applications},
  address   = {New York, NY, USA},
  publisher = {ACM},
  file      = {GrGym:Attachments/GrGym.pdf:application/pdf}
}

@proceedings{.2021b,
  year      = {2021},
  title     = {International Conference on Practical Applications of Agents and Multi-Agent Systems},
  publisher = {{Springer, Cham}},
  file      = {Advances in Practical Applications of Agents Multi-Agent:Attachments/Advances in Practical Applications of Agents Multi-Agent.pdf:application/pdf}
}

@book{.2021c,
  year  = {2021},
  title = {Interactive Robust Policy Optimization for Multi-Agent Reinforcement Learning},
  url   = {https://openreview.net/forum?id=edjy_h3_umk},
  file  = {Interactive Robust Policy Optimization for Multi-Agent R:Attachments/Interactive Robust Policy Optimization for Multi-Agent R.pdf:application/pdf}
}

@book{.2021d,
  year      = {2021},
  title     = {Advances in Data Science and Information Engineering},
  publisher = {{Springer, Cham}},
  file      = {Advances in Data Science and Information Engineering:Attachments/Advances in Data Science and Information Engineering.pdf:application/pdf}
}


@book{.2021e,
  year  = {2021},
  title = {Adversarial Swarm Defense with Decentralized Swarms},
  url   = {https://digitalassets.lib.berkeley.edu/techreports/ucb/incoming/eecs-2021-81.pdf},
  file  = {Adversarial Swarm Defense with Decentralized Swarms:Attachments/Adversarial Swarm Defense with Decentralized Swarms.pdf:application/pdf}
}


@proceedings{.2022,
  year      = {2022},
  title     = {2022 IEEE/AIAA 41st Digital Avionics Systems Conference (DASC)},
  publisher = {IEEE}
}


@proceedings{.2022,
  year      = {2022},
  title     = {2022 13th International Conference on Computing Communication and Networking Technologies (ICCCNT)},
  publisher = {IEEE}
}


@article{.2023,
  year    = {2023},
  title   = {Sim-to-Lab-to-Real: Safe reinforcement learning with shielding and generalization guarantees},
  url     = {https://www.sciencedirect.com/science/article/pii/s0004370222001515},
  pages   = {103811},
  volume  = {314},
  issn    = {0004-3702},
  journal = {Artificial Intelligence},
  doi     = {10.1016/j.artint.2022.103811},
  file    = {Sim-to-Lab-to-Real Safe reinforcement learning with shie:Attachments/Sim-to-Lab-to-Real Safe reinforcement learning with shie.pdf:application/pdf}
}


@misc{.2282023,
  abstract = {Mit Google Scholar k{\"o}nnen Sie ganz einfach nach wissenschaftlicher Literatur suchen. Sie k{\"o}nnen nicht nur viele verschiedene Fachrichtungen, sondern auch unterschiedliche Quellen ausw{\"a}hlen, wie beispielsweise Fachartikel, Diplom- und Doktorarbeiten, B{\"u}cher, Zusammenfassungen oder Gerichtsgutachten.},
  year     = {2/28/2023},
  title    = {Google Scholar},
  url      = {https://scholar.google.de/},
  urldate  = {2/28/2023}
}


@misc{.2282023b,
  year    = {2/28/2023},
  title   = {IEEE Xplore},
  url     = {https://ieeexplore.ieee.org/Xplore/home.jsp},
  urldate = {2/28/2023}
}


@proceedings{.5302021652021,
  year      = {5/30/2021 - 6/5/2021},
  title     = {2021 IEEE International Conference on Robotics and Automation (ICRA)},
  publisher = {IEEE},
  isbn      = {978-1-7281-9077-8}
}


@article{10.5555/2017160.2017162,
  author     = {Webster, Jane and Watson, Richard T.},
  title      = {Analyzing the Past to Prepare for the Future: Writing a Literature Review},
  year       = {2002},
  issue_date = {June 2002},
  publisher  = {Society for Information Management and The Management Information Systems Research Center},
  address    = {USA},
  volume     = {26},
  number     = {2},
  issn       = {0276-7783},
  journal    = {MIS Q.},
  month      = {jun},
  pages      = {xiii–xxiii},
  numpages   = {11}
}

@misc{ACMDigitalLibrary.2282023,
  abstract = {ACM Digital Library Home page},
  author   = {{ACM Digital Library}},
  year     = {2/28/2023},
  title    = {ACM Digital Library},
  url      = {https://dl.acm.org/},
  urldate  = {2/28/2023}
}

@incollection{Ahmed.,
  author = {Ahmed, Ibrahim and Quinones-Grueiro, Marcos and Biswas, Gautam},
  title  = {A high-fidelity simulation test-bed for fault-tolerant octo-rotor control using reinforcement learning},
  pages  = {1--10},
  doi    = {10.1109/DASC55683.2022.9925862},
  file   = {A_high-fidelity_simulation_test-bed_for_fault-tolerant_octo-rotor_control_using_reinforcement_learning:Attachments/A_high-fidelity_simulation_test-bed_for_fault-tolerant_octo-rotor_control_using_reinforcement_learning.pdf:application/pdf}
}


@inproceedings{Ahmed.2022,
  author    = {Ahmed, Ibrahim and Quinones-Grueiro, Marcos and Biswas, Gautam},
  title     = {A high-fidelity simulation test-bed for fault-tolerant octo-rotor control using reinforcement learning},
  publisher = {IEEE},
  booktitle = {2022 IEEE/AIAA 41st Digital Avionics Systems Conference (DASC)},
  year      = {2022},
  doi       = {10.1109/dasc55683.2022.9925862}
}


@inproceedings{Alghonaim.5302021652021,
  author    = {Alghonaim, Raghad and Johns, Edward},
  title     = {Benchmarking Domain Randomisation for Visual Sim-to-Real Transfer},
  pages     = {12802--12808},
  publisher = {IEEE},
  isbn      = {978-1-7281-9077-8},
  booktitle = {2021 IEEE International Conference on Robotics and Automation (ICRA)},
  year      = {2021},
  doi       = {10.1109/ICRA48506.2021.9561134},
  file      = {Benchmarking Domain Randomisation for Visual Sim-to-Real:Attachments/Benchmarking Domain Randomisation for Visual Sim-to-Real.pdf:application/pdf}
}


@inproceedings{Ancuti,
  title      = {Enhancing underwater images and videos by fusion},
  author     = {Ancuti, C. and Ancuti, C.O. and Haber, T. and Bekaert, P.},
  booktitle  = {IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  year       = {2012},
  doi        = {10.1109/CVPR.2012.6247661},
  pages      = {81-88},
  abstract   = {This paper describes a novel strategy to enhance underwater videos and images. Built on the fusion principles, our strategy derives the inputs and the weight measures only from the degraded version of the image. In order to overcome the limitations of the underwater medium we define two inputs that represent color corrected and contrast enhanced versions of the original underwater image/frame, but also four weight maps that aim to increase the visibility of the distant objects degraded due to the medium scattering and absorption. Our strategy is a single image approach that does not require specialized hardware or knowledge about the underwater conditions or scene structure. Our fusion framework also supports temporal coherence between adjacent frames by performing an effective edge preserving noise reduction strategy. The enhanced images and videos are characterized by reduced noise level, better exposed-ness of the dark regions, improved global contrast while the finest details and edges are enhanced significantly. In addition, the utility of our enhancing technique is proved for several challenging applications.},
  bdsk-url-1 = {http://dx.doi.org/10.1109/CVPR.2012.6247661},
  issn       = {1063-6919},
  keywords   = {image colour analysis;image denoising;image enhancement;image fusion;light absorption;light scattering;video signal processing;adjacent frames;color corrected version;contrast enhanced version;distant objects degradation;edge preserving noise reduction strategy;enhancing technique;fusion framework;fusion principles;global contrast;image fusion;medium absorption;medium scattering;reduced noise level;single image approach;temporal coherence;underwater conditions;underwater image enhancement;underwater scene structure;underwater videos enhancement;weight maps;Image color analysis;Image edge detection;Image restoration;Laplace equations;Lighting;Noise;Videos}
}


@ARTICLE{Arulkumaran.2017,

  author={Arulkumaran, Kai and Deisenroth, Marc Peter and Brundage, Miles and Bharath, Anil Anthony},
  journal={IEEE Signal Processing Magazine}, 
  title={Deep Reinforcement Learning: A Brief Survey}, 
  year={2017},
  volume={34},
  number={6},
  pages={26-38},
  doi={10.1109/MSP.2017.2743240}
}


@INPROCEEDINGS{Ayala.2020,
  author={Ayala, Angel and Cruz, Francisco and Campos, Diego and Rubio, Rodrigo and Fernandes, Bruno and Dazeley, Richard},
  booktitle={2020 Joint IEEE 10th International Conference on Development and Learning and Epigenetic Robotics (ICDL-EpiRob)}, 
  title={A Comparison of Humanoid Robot Simulators: A Quantitative Approach}, 
  year={2020},
  volume={},
  number={},
  pages={1-6},
  doi={10.1109/ICDL-EpiRob48136.2020.9278116}
}


@article{Bellman.1966,
  doi = {doi:10.1126/science.153.3731.34},
  author = {Richard Bellman},
  title = {Dynamic Programming},
  journal = {Science},
  volume = {153},
  number = {3731},
  pages = {34-37},
  year = {1966},
  doi = {10.1126/science.153.3731.34},
  URL = {https://www.science.org/doi/abs/10.1126/science.153.3731.34},
  eprint = {https://www.science.org/doi/pdf/10.1126/science.153.3731.34},
}


@article{Bernstein2014,
  abstract  = {This issue's "Cloud Tidbit" focuses on container technology and how it's emerging as an important part of the cloud computing infrastructure. It looks at Docker, an open source project that automates the faster deployment of Linux applications, and Kubernetes, an open source cluster manager for Docker containers.},
  author    = {Bernstein, David},
  doi       = {10.1109/MCC.2014.51},
  issn      = {23256095},
  journal   = {IEEE Cloud Computing},
  keywords  = {cloud,containers,dockers,virtual machines},
  number    = {3},
  pages     = {81--84},
  publisher = {Published by the IEEE Computer Society},
  title     = {{Containers and cloud: From LXC to docker to kubernetes}},
  volume    = {1},
  year      = {2014}
}


@inproceedings{Bharadhwaj.2019,
  author    = {Bharadhwaj, Homanga and Wang, Zihan and Bengio, Yoshua and Paull, Liam},
  title     = {A Data-Efficient Framework for Training and Sim-to-Real Transfer of Navigation Policies},
  publisher = {IEEE},
  booktitle = {2019 International Conference on Robotics and Automation (ICRA)},
  year      = {2019},
  doi       = {10.1109/icra.2019.8794310}
}


@inproceedings{Bharadhwaj.2019,
  author    = {Bharadhwaj, Homanga and Wang, Zihan and Bengio, Yoshua and Paull, Liam},
  title     = {A Data-Efficient Framework for Training and Sim-to-Real Transfer of Navigation Policies},
  publisher = {IEEE},
  booktitle = {2019 International Conference on Robotics and Automation (ICRA)},
  year      = {2019},
  doi       = {10.1109/icra.2019.8794310}
}


@manual{biblatex:manual,
  title        = {The Biblatex Package},
  author       = {Philipp Lehman},
  year         = {2014},
  edition      = {Version 2.9a},
  organization = {CTAN.org},
  url          = {http://ctan.org/pkg/biblatex},
  bdsk-url-1   = {http://ctan.org/pkg/biblatex},
  owner        = {tobiasstraub},
  timestamp    = {2015.01.09}
}


@thesis{Boettger:Diplomarbeit,
  title       = {Cloud Computing richtig gemacht: Ein Vorgehensmodell zur Auswahl von SaaS-Anwendungen: Am Beispiel eines hybriden Cloud-Ansatzes für Vertriebssoftware in KMU},
  author      = {Böttger, Markus},
  institution = {Universität Stuttgart},
  type        = {Diplomarbeit},
  year        = {2012},
  edition     = {1. Auflage},
  isbn        = {978-384-28281-7-9},
  keywords    = {Cloud computing. Hybrider Betrieb. On-Premise Software},
  publisher   = {Diplomica Verlag GmbH}
}


@article{Brockman.2016,
  author    = {Greg Brockman and
               Vicki Cheung and
               Ludwig Pettersson and
               Jonas Schneider and
               John Schulman and
               Jie Tang and
               Wojciech Zaremba},
  title     = {OpenAI Gym},
  journal   = {CoRR},
  volume    = {abs/1606.01540},
  year      = {2016},
  url       = {http://arxiv.org/abs/1606.01540},
  eprinttype = {arXiv},
  eprint    = {1606.01540},
  timestamp = {Fri, 08 Nov 2019 12:51:06 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/BrockmanCPSSTZ16.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}


@book{BuschlingerStaab,
  title     = {{UNIX} für Software-Entwickler - Konzepte, Werkzeuge und Ideen},
  author    = {Frank Staab},
  year      = {1993},
  isbn      = {978-3-519-02290-9},
  publisher = {Teubner},
  bibsource = {dblp computer science bibliography, http://dblp.org},
  biburl    = {http://dblp.uni-trier.de/rec/bib/books/daglib/0069663}
}

@article{Canese.2021,
  abstract = {In this review, we present an analysis of the most used multi-agent reinforcement learning algorithms. Starting with the single-agent reinforcement learning algorithms, we focus on the most critical issues that must be taken into account in their extension to multi-agent scenarios. The analyzed algorithms were grouped according to their features. We present a detailed taxonomy of the main multi-agent approaches proposed in the literature, focusing on their related mathematical models. For each algorithm, we describe the possible application fields, while pointing out its pros and cons. The described multi-agent algorithms are compared in terms of the most important characteristics for multi-agent reinforcement learning applications---namely, nonstationarity, scalability, and observability. We also describe the most common benchmark environments used to evaluate the performances of the considered methods.},
  author   = {Canese, Lorenzo and Cardarilli, Gian Carlo and {Di Nunzio}, Luca and Fazzolari, Rocco and Giardino, Daniele and Re, Marco and Span{\`o}, Sergio},
  year     = {2021},
  title    = {Multi-Agent Reinforcement Learning: A Review of Challenges and Applications},
  url      = {https://www.mdpi.com/2076-3417/11/11/4948},
  keywords = {machine learning;multi-agent;reinforcement learning;swarm},
  pages    = {4948},
  volume   = {11},
  number   = {11},
  journal  = {Applied Sciences},
  doi      = {10.3390/app11114948},
  file     = {Multi-Agent Reinforcement Learning A Review of Challenge:Attachments/Multi-Agent Reinforcement Learning A Review of Challenge.pdf:application/pdf}
}

@article{Carvalho:PJ:2012-1,
  title      = {Documenting ITIL processes with LaTeX (Portuguese)},
  author     = {Rayans Carvalho and Francisco Reinaldo},
  year       = {2012},
  number     = {1},
  url        = {http://tug.org/pracjourn/2012-1/rayans},
  bdsk-url-1 = {http://tug.org/pracjourn/2012-1/rayans},
  journal    = {The Prac\TeX\ Journal}
}

@article{COLLINS2022101217,
  title    = {Power TAC: Software architecture for a competitive simulation of sustainable smart energy markets},
  journal  = {SoftwareX},
  volume   = {20},
  pages    = {101217},
  year     = {2022},
  issn     = {2352-7110},
  doi      = {https://doi.org/10.1016/j.softx.2022.101217},
  url      = {https://www.sciencedirect.com/science/article/pii/S2352711022001352},
  author   = {John Collins and Wolfgang Ketter},
  keywords = {Competitive benchmarking, Simulation, Open-source software, Sustainable electricity, Smart markets, Trading agents},
  abstract = {Power TAC (www.powertac.org) is a discrete-time competitive simulation that models a retail electricity market. Since 2012 it has been the foundation of an annual competition, challenging teams from around the world to build autonomous trading agents that communicate with the simulation over the internet. These “retail brokers” offer energy services to customers through tariff contracts, and must then serve those customers by trading in a wholesale market. Hourly differences between wholesale market positions and net consumption by their subscribed customers must be cleared in a local balancing market using a combination of demand response and wholesale balancing energy. The simulation server is open source and highly modular, designed to be accessible to inexperienced student developers. It makes heavy use of annotations and aspect-oriented programming to achieve consistency and ensure that all important events are recorded, allowing simulations to be re-played and analyzed in depth.}
}


@inproceedings{Cutler.2014,
  author    = {Cutler, Mark and Walsh, Thomas J. and How, Jonathan P.},
  title     = {Reinforcement learning with multi-fidelity simulators},
  publisher = {IEEE},
  booktitle = {2014 IEEE International Conference on Robotics and Automation (ICRA)},
  year      = {2014},
  doi       = {10.1109/icra.2014.6907423}
}


@article{DBLP:journals/corr/abs-1910-10537,
  author     = {Reda Bahi Slaoui and
                William R. Clements and
                Jakob N. Foerster and
                S{\'{e}}bastien Toth},
  title      = {Robust Domain Randomization for Reinforcement Learning},
  journal    = {CoRR},
  volume     = {abs/1910.10537},
  year       = {2019},
  url        = {http://arxiv.org/abs/1910.10537},
  eprinttype = {arXiv},
  eprint     = {1910.10537},
  timestamp  = {Fri, 25 Oct 2019 14:59:26 +0200},
  biburl     = {https://dblp.org/rec/journals/corr/abs-1910-10537.bib},
  bibsource  = {dblp computer science bibliography, https://dblp.org}
}


@misc{Deshpande.2020,
 abstract = {In this paper, we present a novel developmental reinforcement learning-based controller for a quadcopter with thrust vectoring capabilities. This multirotor UAV design has tilt-enabled rotors. It utilizes the rotor force magnitude and direction to achieve the desired state during flight. The control policy of this robot is learned using the policy transfer from the learned controller of the quadcopter (comparatively simple UAV design without thrust vectoring). This approach allows learning a control policy for systems with multiple inputs and multiple outputs. The performance of the learned policy is evaluated by physics-based simulations for the tasks of hovering and way-point navigation. The flight simulations utilize a flight controller based on reinforcement learning without any additional PID components. The results show faster learning with the presented approach as opposed to learning the control policy from scratch for this new UAV design created by modifications in a conventional quadcopter, i.e., the addition of more degrees of freedom (4-actuators in conventional quadcopter to 8-actuators in tilt-rotor quadcopter). We demonstrate the robustness of our learned policy by showing the recovery of the tilt-rotor platform in the simulation from various non-static initial conditions in order to reach a desired state. The developmental policy for the tilt-rotor UAV also showed superior fault tolerance when compared with the policy learned from the scratch. The results show the ability of the presented approach to bootstrap the learned behavior from a simpler system (lower-dimensional action-space) to a more complex robot (comparatively higher-dimensional action-space) and reach better performance faster.},
 author = {Deshpande, Aditya M. and Kumar, Rumit and Minai, Ali A. and Kumar, Manish},
 date = {2020},
 title = {Developmental Reinforcement Learning of Control Policy of a Quadcopter UAV with Thrust Vectoring Rotors},
 url = {https://arxiv.org/pdf/2007.07793},
 keywords = {Artificial Intelligence (cs.AI);Machine Learning (cs.LG);Robotics (cs.RO);Systems and Control (eess.SY)},
 file = {Developmental Reinforcement Learning of Control Policy o:Attachments/Developmental Reinforcement Learning of Control Policy o.pdf:application/pdf}
}

@article{Deshpande.2021,
 author = {Deshpande, Aditya M. and Minai, Ali A. and Kumar, Manish},
 year = {2021},
 title = {Robust Deep Reinforcement Learning for Quadcopter Control},
 pages = {90--95},
 volume = {54},
 number = {20},
 issn = {24058963},
 journal = {IFAC-PapersOnLine},
 doi = {10.1016/j.ifacol.2021.11.158},
 file = {Robust Deep Reinforcement Learning for Quadcopter Control:Attachments/Robust Deep Reinforcement Learning for Quadcopter Control.pdf:application/pdf}
}


@misc{Endres,
  author    = {Endres, Cornelia},
  booktitle = {Bachelor Print},
  title     = {{Das Experteninterview – Leitfaden f{\"{u}}r die Bachelorarbeit}},
  urldate   = {2019-08-09}
}


@article{FabioMuratore.2018,
  abstract = {Domain Randomization for Simulation-Based Policy Optimization with Transferability AssessmentFabio Muratore,~Felix Treede,~Michael Gienger,~Ja...},
  author   = {{Fabio Muratore} and {Felix Treede} and {Michael Gienger} and {Jan Peters}},
  year     = {2018},
  title    = {Domain Randomization for Simulation-Based Policy Optimization with Transferability Assessment},
  url      = {http://proceedings.mlr.press/v87/muratore18a},
  pages    = {700--713},
  issn     = {2640-3498},
  journal  = {Conference on Robot Learning},
  file     = {Domain Randomization for Simulation-Based Policy Optimiz:Attachments/Domain Randomization for Simulation-Based Policy Optimiz.pdf:application/pdf}
}

@incollection{Farahani.2021,
  abstract  = {Classical machine learning assumes that the training and test sets come from the same distributions. Therefore, a model learned from the labeled training data is expected to perform well on the test data. However, this assumption may not always hold in real-world...},
  author    = {Farahani, Abolfazl and Voghoei, Sahar and Rasheed, Khaled and Arabnia, Hamid R.},
  title     = {A Brief Review of Domain Adaptation},
  url       = {https://link.springer.com/chapter/10.1007/978-3-030-71704-9_65#chapter-info},
  pages     = {877--894},
  publisher = {{Springer, Cham}},
  booktitle = {Advances in Data Science and Information Engineering},
  year      = {2021},
  doi       = {10.1007/978-3-030-71704-9{\textunderscore }65}
}


@article{Foronda.2021,
  author  = {Foronda, Cynthia L.},
  year    = {2021},
  title   = {What Is Virtual Simulation?},
  pages   = {8},
  volume  = {52},
  issn    = {18761399},
  journal = {Clinical Simulation in Nursing},
  doi     = {10.1016/j.ecns.2020.12.004},
  file    = {What Is Virtual Simulation :Attachments/What Is Virtual Simulation .pdf:application/pdf}
}

@Inbook{Furrer.2016,
 author="Furrer, Fadri
 and Burri, Michael
 and Achtelik, Markus
 and Siegwart, Roland",
 editor="Koubaa, Anis",
 title="RotorS---A Modular Gazebo MAV Simulator Framework",
 bookTitle="Robot Operating System (ROS): The Complete Reference (Volume 1)",
 year="2016",
 publisher="Springer International Publishing",
 address="Cham",
 pages="595--625",
 abstract="In this chapter we present a modular Micro Aerial Vehicle (MAV) simulation framework, which enables a quick start to perform research on MAVs. After reading this chapter, the reader will have a ready to use MAV simulator, including control and state estimation. The simulator was designed in a modular way, such that different controllers and state estimators can be used interchangeably, while incorporating new MAVs is reduced to a few steps. The provided controllers can be adapted to a custom vehicle by only changing a parameter file. Different controllers and state estimators can be compared with the provided evaluation framework. The simulation framework is a good starting point to tackle higher level tasks, such as collision avoidance, path planning, and vision based problems, like Simultaneous Localization and Mapping (SLAM), on MAVs. All components were designed to be analogous to its real world counterparts. This allows the usage of the same controllers and state estimators, including their parameters, in the simulation as on the real MAV.",
 isbn="978-3-319-26054-9",
 doi="10.1007/978-3-319-26054-9_23",
 url="https://doi.org/10.1007/978-3-319-26054-9_23"
}

@phdthesis{GuttenPlag,
  title       = {Verfassung und Verfassungsvertrag : konstitutionelle Entwicklungsstufen in den USA und der EU},
  author      = {Karl-Theodor {zu Guttenberg}},
  institution = {Universität Bayreuth},
  year        = {2009},
  note        = {Doktorgrad am 23.2.2011 aberkannt},
  type        = {Dissertation},
  owner       = {tobiasstraub},
  timestamp   = {2015.01.09}
}

@article{Haarnoja.2018,
  author    = {Tuomas Haarnoja and
               Aurick Zhou and
               Pieter Abbeel and
               Sergey Levine},
  title     = {Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning
               with a Stochastic Actor},
  journal   = {CoRR},
  volume    = {abs/1801.01290},
  year      = {2018},
  url       = {http://arxiv.org/abs/1801.01290},
  eprinttype = {arXiv},
  eprint    = {1801.01290},
  timestamp = {Mon, 13 Aug 2018 16:48:10 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1801-01290.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{Hentati.2018,
  author    = {Hentati, Aicha Idriss and Krichen, Lobna and Fourati, Mohamed and Fourati, Lamia Chaari},
  title     = {Simulation Tools, Environments and Frameworks for UAV Systems Performance Analysis},
  publisher = {IEEE},
  booktitle = {2018 14th International Wireless Communications {\&} Mobile Computing Conference (IWCMC)},
  year      = {2018},
  doi       = {10.1109/iwcmc.2018.8450505},
  file      = {Simulation_Tools_Environments_and_Frameworks_for_UAV_Systems_Performance_Analysis:Attachments/Simulation_Tools_Environments_and_Frameworks_for_UAV_Systems_Performance_Analysis.pdf:application/pdf}
}

@article{hitzler2011optimierung,
  title   = {Optimierung und Intensivierung des Einsatzes von Planspielen an Hochschulen},
  author  = {Trautwein, Friedrich},
  year    = {2011},
  pages   = {101--125},
  comment = {weitere Autoren: Hitzler, Sebastian and Z{\"u}rn, Birgit and},
  journal = {Planspiele--Qualit{\"a}t und Innovation: Neue Ans{\"a}tze aus Theorie und Praxis, hrsg. von: Hitzler, S}
}

@article{Holzweiig.2022,
  author = {Holzwei{\ss}ig, Kai},
  title  = {Wissenschaftliches Arbeiten},
  year   = {2022},
  volume = {6},
  number = {6},
  file   = {Wissenschaftliches-Arbeiten:Attachments/Wissenschaftliches-Arbeiten.pdf:application/pdf}
}

@article{Huang.2017,
  author    = {Sandy H. Huang and
               Nicolas Papernot and
               Ian J. Goodfellow and
               Yan Duan and
               Pieter Abbeel},
  title     = {Adversarial Attacks on Neural Network Policies},
  journal   = {CoRR},
  volume    = {abs/1702.02284},
  year      = {2017},
  url       = {http://arxiv.org/abs/1702.02284},
  eprinttype = {arXiv},
  eprint    = {1702.02284},
  timestamp = {Mon, 13 Aug 2018 16:48:47 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/HuangPGDA17.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@misc{Liu.2023,
      title={On the Robustness of Safe Reinforcement Learning under Observational Perturbations}, 
      author={Zuxin Liu and Zijian Guo and Zhepeng Cen and Huan Zhang and Jie Tan and Bo Li and Ding Zhao},
      year={2023},
      eprint={2205.14691},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{Pullum.2022,
      title={Review of Metrics to Measure the Stability, Robustness and Resilience of Reinforcement Learning}, 
      author={Laura L. Pullum},
      year={2022},
      eprint={2203.12048},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@Article{Moos.2022,
  AUTHOR = {Moos, Janosch and Hansel, Kay and Abdulsamad, Hany and Stark, Svenja and Clever, Debora and Peters, Jan},
  TITLE = {Robust Reinforcement Learning: A Review of Foundations and Recent Advances},
  JOURNAL = {Machine Learning and Knowledge Extraction},
  VOLUME = {4},
  YEAR = {2022},
  NUMBER = {1},
  PAGES = {276--315},
  URL = {https://www.mdpi.com/2504-4990/4/1/13},
  ISSN = {2504-4990},
  ABSTRACT = {Reinforcement learning (RL) has become a highly successful framework for learning in Markov decision processes (MDP). Due to the adoption of RL in realistic and complex environments, solution robustness becomes an increasingly important aspect of RL deployment. Nevertheless, current RL algorithms struggle with robustness to uncertainty, disturbances, or structural changes in the environment. We survey the literature on robust approaches to reinforcement learning and categorize these methods in four different ways: (i) Transition robust designs account for uncertainties in the system dynamics by manipulating the transition probabilities between states; (ii) Disturbance robust designs leverage external forces to model uncertainty in the system behavior; (iii) Action robust designs redirect transitions of the system by corrupting an agent&rsquo;s output; (iv) Observation robust designs exploit or distort the perceived system state of the policy. Each of these robust designs alters a different aspect of the MDP. Additionally, we address the connection of robustness to the risk-based and entropy-regularized RL formulations. The resulting survey covers all fundamental concepts underlying the approaches to robust reinforcement learning and their recent advances.},
  DOI = {10.3390/make4010013}
}

@book{Indrasiri2018,
  abstract  = {Includes index. "Microservices for the Enterprise covers state-of-the-art techniques around microservices messaging, service development and description, service discovery, governance, and data management technologies and guides you through the microservices design process. Also included is the importance of organizing services as core versus atomic, composite versus integration, and API versus edge, and how such organization helps to eliminate the use of a central ESB and expose services through an API gateway"-- The case for microservices -- Designing microservices -- Inter-service communication -- Developing services -- Data management -- Microservices governance -- Integrating microservices -- Deploying and running microservices -- Service mesh -- APIs, events, and streams -- Microservices security fundamentals -- Securing microservices -- Observability.},
  author    = {Indrasiri, Kasun and Siriwardena, Prabath},
  booktitle = {Microservices for the Enterprise},
  doi       = {10.1007/978-1-4842-3858-5},
  isbn      = {9781484238578},
  title     = {{Microservices for the Enterprise}},
  year      = {2018}
}

@misc{Ivaldi.2272014,
  abstract = {The number of tools for dynamics simulation has grown in the last years. It is necessary for the robotics community to have elements to ponder which of the available tools is the best for their research. As a complement to an objective and quantitative comparison, difficult to obtain since not all the tools are open-source, an element of evaluation is user feedback. With this goal in mind, we created an online survey about the use of dynamical simulation in robotics. This paper reports the analysis of the participants' answers and a descriptive information fiche for the most relevant tools. We believe this report will be helpful for roboticists to choose the best simulation tool for their researches.},
  author   = {Ivaldi, Serena and Padois, Vincent and Nori, Francesco},
  date     = {2/27/2014},
  year     = {2014},
  title    = {Tools for dynamics simulation of robots: a survey based on user feedback},
  url      = {https://arxiv.org/pdf/1402.7050},
  keywords = {Computer Science - Robotics;Robotics (cs.RO)},
  file     = {Tools for dynamics simulation of robots a survey based o:Attachments/Tools for dynamics simulation of robots a survey based o.pdf:application/pdf}
}

@book{keinAutorAberHerausgeber,
  title     = {Buch ohne Autor, aber mit Herausgeber},
  year      = {2020},
  editor    = {Max Meier},
  location  = {Bielefeld},
  publisher = {Nonsens-Verlag}
}

@article{Koch.2018,
  author    = {William Koch and
               Renato Mancuso and
               Richard West and
               Azer Bestavros},
  title     = {Reinforcement Learning for {UAV} Attitude Control},
  journal   = {CoRR},
  volume    = {abs/1804.04154},
  year      = {2018},
  url       = {http://arxiv.org/abs/1804.04154},
  eprinttype = {arXiv},
  eprint    = {1804.04154},
  timestamp = {Mon, 13 Aug 2018 16:48:14 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1804-04154.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@misc{Korber.2021,
  abstract = {This letter compares the performance of four different, popular simulation environments for robotics and reinforcement learning (RL) through a series of benchmarks. The benchmarked scenarios are designed carefully with current industrial applications in mind. Given the need to run simulations as fast as possible to reduce the real-world training time of the RL agents, the comparison includes not only different simulation environments but also different hardware configurations, ranging from an entry-level notebook up to a dual CPU high performance server. We show that the chosen simulation environments benefit the most from single core performance. Yet, using a multi core system, multiple simulations could be run in parallel to increase the performance.},
  author   = {K{\"o}rber, Marian and Lange, Johann and Rediske, Stephan and Steinmann, Simon and Gl{\"u}ck, Roland},
  date     = {2021},
  title    = {Comparing Popular Simulation Environments in the Scope of Robotics and Reinforcement Learning},
  url      = {https://arxiv.org/pdf/2103.04616},
  keywords = {Artificial Intelligence (cs.AI);Machine Learning (cs.LG);Robotics (cs.RO)},
  file     = {Comparing Popular Simulation Environments in the Scope o:Attachments/Comparing Popular Simulation Environments in the Scope o.pdf:application/pdf}
}

@book{Koubaa.2016,
 abstract = {Volume 1 includes twenty-seven chapters organized into eight parts. Part 1 presents the basics and foundations of ROS. In Part 2, four chapters deal with navigation, motion and planning. Part 3 provides four examples of service and experimental robots. Part 4 deals with real-world deployment of applications. Part 5 presents signal-processing tools for perception and sensing. Part 6 provides software engineering methodologies to design complex software with ROS. Simulations frameworks are presented in Part 7. Finally, Part 8 presents advanced tools and frameworks for ROS including multi-master extension, network introspection, controllers and cognitive systems. The second volume is a continuation of the successful first volume of this Springer book, and as well as addressing broader topics it puts a particular focus on unmanned aerial vehicles (UAVs) with Robot Operating System (ROS). Consisting of three types of chapters: tutorials, cases studies, and research papers, it provides comprehensive additional material on ROS and the aspects of developing robotics systems, algorithms, frameworks, and applications with ROS. The objective of this third volume is to provide readers with additional and comprehensive coverage of the ROS and an overview of the latest achievements, trends and packages developed with and for it. The fourth volume is divided into four parts: Part 1 features two papers on navigation, discussing SLAM and path planning. Part 2 focuses on the integration of ROS into quadcopters and their control. Part 3 then discusses two emerging applications for robotics: cloud robotics, and video stabilization. Part 4 presents tools developed for ROS; the first is a practical alternative to the roslaunch system, and the second is related to penetration testing.},
 year = {2016},
 title = {Robot operating system (ROS): The complete reference},
 keywords = {Robots},
 address = {Cham},
 volume = {volume 625, 707, 778, 831},
 publisher = {Springer},
 isbn = {9783030201906},
 series = {Studies in Computational Intelligence},
 editor = {Koub{\^a}a, Anis},
 file = {978-3-319-26054-9:Attachments/978-3-319-26054-9.pdf:application/pdf}
}

@online{langeURL,
  title  = {Lange URL als Herausforderung für den Zeilenumbruch},
  author = {Google},
  url    = {http://www.google.de/search?q=biblatex+umbruch+url&hl=de&gbv=2&oq=biblatex+umbruch+url&gs_l=heirloom-serp.12...0.0.0.6831.0.0.0.0.0.0.0.0..0.0.msedr...0...1ac..34.heirloom-serp..0.0.0.5959BWSvzu0},
  year   = {o.J.}
}

@misc{Li.2019,
  doi       = {10.48550/ARXIV.1908.06973},
  url       = {https://arxiv.org/abs/1908.06973},
  author    = {Li, Yuxi},
  keywords  = {Machine Learning (cs.LG), Artificial Intelligence (cs.AI), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title     = {Reinforcement Learning Applications},
  publisher = {arXiv},
  year      = {2019},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@inbook{Mann,
  title     = {Beyond Systematic Innovation},
  author    = {Mann, D.},
  editor    = {Jöstingmeier, B. and Boeddrich, H.-J.},
  pages     = {45--61},
  publisher = {DUV},
  year      = {2005},
  address   = {Wiesbaden},
  booktitle = {Cross-Cultural Innovation},
  owner     = {tobiasstraub},
  timestamp = {2015.01.04}
}

@inproceedings{Maria.1997,
  author    = {Maria, Anu},
  title     = {Introduction to modeling and simulation},
  publisher = {{ACM Press}},
  booktitle = {Proceedings of the 29th conference on Winter simulation  - WSC '97},
  year      = {1997},
  address   = {New York, New York, USA},
  doi       = {10.1145/268437.268440},
  file      = {INTRODUCTION TO MODELING AND SIMULATION:Attachments/INTRODUCTION TO MODELING AND SIMULATION.pdf:application/pdf}
}

@report{mayer:PA1,
  title       = {Automatisierung von Zellformatierungen in Excel},
  author      = {Lieschen Mayer},
  institution = {DHBW Stuttgart},
  type        = {1. Projektarbeit},
  year        = {2015},
  subtitle    = {Entwicklung eines Prototypen mit VBA},
  owner       = {tobiasstraub},
  timestamp   = {2015.01.28}
}

@report{mayerLukas:PA1,
  title       = {Unternehmenskommunikation mittels RFC822},
  author      = {Lukas Mayer},
  institution = {DHBW Stuttgart},
  type        = {1. Projektarbeit},
  year        = {2015}
}

@book{Mayring2002,
  abstract  = {Qualitative Forschung ist keine beliebig einsetzbare Technik, sondern eine Grundhaltung, ein Denkstil, der immer streng am Gegenstand orientiert ist. Das Buch stellt Bez{\"{u}}ge zum Gegenstandsfeld her und m{\"{o}}chte einer Trennung zwischen Gegenstandsspezialisten und Methodenspezialisten entgegenwirken. Es bietet Unterst{\"{u}}tzung bei der {\"{U}}berpr{\"{u}}fung der Aussagekraft von Projekten und deren Methodik.},
  address   = {Weinheim, Basel},
  author    = {Mayring, Philipp},
  edition   = {6. Auflage},
  isbn      = {978-3-407-29093-9},
  pages     = {170},
  publisher = {Beltz},
  title     = {{Einf{\"{u}}hrung in die qualitative Sozialforschung}},
  year      = {2002}
}

@techreport{Mell2011,
  abstract  = {The National Institute of Standards and Technology (NIST) developed this document in furtherance of its statutory responsibilities under the Federal Information Security Management Act (FISMA) of 2002, Public Law 107-347. NIST is responsible for developing standards and guidelines, including minimum requirements, for providing adequate information security for all agency operations and assets; but such standards and guidelines shall not apply to national security systems. This guideline is consistent with the requirements of the Office of Management and Budget (OMB) Circular A-130, Section 8b(3), Securing Agency Information Systems, as analyzed in A-130, Appendix IV: Analysis of Key Sections. Supplemental information is provided in A-130, Appendix III. This guideline has been prepared for use by Federal agencies. It may be used by nongovernmental organizations on a voluntary basis and is not subject to copyright, though attribution is desired. Nothing in this document should be taken to contradict standards and guidelines made mandatory and binding on Federal agencies by the Secretary of Commerce under statutory authority, nor should these guidelines be interpreted as altering or superseding the existing authorities of the Secretary of Commerce, Director of the OMB, or any other Federal official.},
  author    = {Mell, Peter and Grance, Timothy},
  booktitle = {National Institute of Standard and Technology},
  publisher = {U.S. Department of Commerce},
  title     = {{The NIST Definition of Cloud Computing - Special Publication 800-145}},
  url       = {http://faculty.winthrop.edu/domanm/csci411/Handouts/NIST.pdf},
  year      = {2011}
}

@book{MitDreiAutoren,
  title     = {Test},
  author    = {Max Muller and Mayer, Laura and Schulze, Werner},
  year      = {2013},
  owner     = {tobiasstraub},
  timestamp = {2015.01.04}
}

@book{MitZweiAutoren,
  title  = {Test},
  author = {Max Müller and Mayer, Lieschen},
  year   = {2013}
}

@article{Mnih.2013,
  author    = {Volodymyr Mnih and
               Koray Kavukcuoglu and
               David Silver and
               Alex Graves and
               Ioannis Antonoglou and
               Daan Wierstra and
               Martin A. Riedmiller},
  title     = {Playing Atari with Deep Reinforcement Learning},
  journal   = {CoRR},
  volume    = {abs/1312.5602},
  year      = {2013},
  url       = {http://arxiv.org/abs/1312.5602},
  eprinttype = {arXiv},
  eprint    = {1312.5602},
  timestamp = {Mon, 13 Aug 2018 16:47:42 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/MnihKSGAWR13.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{Mnih.2016,
  author    = {Volodymyr Mnih and
               Adri{\`{a}} Puigdom{\`{e}}nech Badia and
               Mehdi Mirza and
               Alex Graves and
               Timothy P. Lillicrap and
               Tim Harley and
               David Silver and
               Koray Kavukcuoglu},
  title     = {Asynchronous Methods for Deep Reinforcement Learning},
  journal   = {CoRR},
  volume    = {abs/1602.01783},
  year      = {2016},
  url       = {http://arxiv.org/abs/1602.01783},
  eprinttype = {arXiv},
  eprint    = {1602.01783},
  timestamp = {Mon, 13 Aug 2018 16:47:40 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/MnihBMGLHSK16.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{Molchanov.2019,
  author    = {Artem Molchanov and
               Tao Chen and
               Wolfgang H{\"{o}}nig and
               James A. Preiss and
               Nora Ayanian and
               Gaurav S. Sukhatme},
  title     = {Sim-to-(Multi)-Real: Transfer of Low-Level Robust Control Policies
               to Multiple Quadrotors},
  journal   = {CoRR},
  volume    = {abs/1903.04628},
  year      = {2019},
  url       = {http://arxiv.org/abs/1903.04628},
  eprinttype = {arXiv},
  eprint    = {1903.04628},
  timestamp = {Sun, 31 Mar 2019 19:01:24 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1903-04628.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{Muratore.2021,
  author  = {Muratore, Fabio and Eilers, Christian and Gienger, Michael and Peters, Jan},
  year    = {2021},
  title   = {Data-Efficient Domain Randomization With Bayesian Optimization},
  pages   = {911--918},
  volume  = {6},
  number  = {2},
  journal = {IEEE Robotics and Automation Letters},
  doi     = {10.1109/LRA.2021.3052391},
  file    = {Data-Efficient Domain Randomization With Bayesian Optimi:Attachments/Data-Efficient Domain Randomization With Bayesian Optimi.pdf:application/pdf}
}

@article{Nguyen.2020,
  author  = {Nguyen, Thanh Thi and Nguyen, Ngoc Duy and Vamplew, Peter and Nahavandi, Saeid and Dazeley, Richard and Lim, Chee Peng},
  year    = {2020},
  title   = {A multi-objective deep reinforcement learning framework},
  pages   = {103915},
  volume  = {96},
  issn    = {09521976},
  journal = {Engineering Applications of Artificial Intelligence},
  doi     = {10.1016/j.engappai.2020.103915},
  file    = {A multi-objective deep reinforcement learning framework:Attachments/A multi-objective deep reinforcement learning framework.pdf:application/pdf}
}

@inproceedings{Ningombam.2022,
  author    = {Ningombam, Devarani Devi},
  title     = {Deep Reinforcement Learning Algorithms for Machine-to-Machine Communications: A Review},
  publisher = {IEEE},
  booktitle = {2022 13th International Conference on Computing Communication and Networking Technologies (ICCCNT)},
  year      = {2022},
  doi       = {10.1109/icccnt54827.2022.9984457},
  file      = {Deep_Reinforcement_Learning_Algorithms_for_Machine-to-Machine_Communications_A_Review:Attachments/Deep_Reinforcement_Learning_Algorithms_for_Machine-to-Machine_Communications_A_Review.pdf:application/pdf}
}

@book{OhneAutoren,
  title  = {UnbekannterAutor},
  author = {o.V.},
  year   = {2016}
}


@book{OhneAutorenOhneJahr,
  title     = {UnbekannterAutor, unbekannterTitel},
  author    = {o.V.},
  owner     = {tobiasstraub},
  timestamp = {2015.01.04}
}

@book{OhneAutorenOhneJahr2,
  title     = {UnbekannterAutor, unbekannterTitel die Zweite},
  author    = {o.V.},
  owner     = {tobiasstraub},
  timestamp = {2015.01.04}
}

@ARTICLE{Pan.2021,
       author = {{Pan}, Alexander and {Lee}, Yongkyun and {Zhang}, Huan and {Chen}, Yize and {Shi}, Yuanyuan},
        title = "{Improving Robustness of Reinforcement Learning for Power System Control with Adversarial Training}",
      journal = {arXiv e-prints},
     keywords = {Electrical Engineering and Systems Science - Systems and Control, Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
         year = 2021,
        month = oct,
          eid = {arXiv:2110.08956},
        pages = {arXiv:2110.08956},
          doi = {10.48550/arXiv.2110.08956},
archivePrefix = {arXiv},
       eprint = {2110.08956},
 primaryClass = {eess.SY},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2021arXiv211008956P},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@INPROCEEDINGS{Tobin.2017,
  author={Tobin, Josh and Fong, Rachel and Ray, Alex and Schneider, Jonas and Zaremba, Wojciech and Abbeel, Pieter},
  booktitle={2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
  title={Domain randomization for transferring deep neural networks from simulation to the real world},
  year={2017},
  volume={},
  number={},
  pages={23-30},
  doi={10.1109/IROS.2017.8202133}
}

@article{Chen.2021,
  author    = {Xiaoyu Chen and
               Jiachen Hu and
               Chi Jin and
               Lihong Li and
               Liwei Wang},
  title     = {Understanding Domain Randomization for Sim-to-real Transfer},
  journal   = {CoRR},
  volume    = {abs/2110.03239},
  year      = {2021},
  url       = {https://arxiv.org/abs/2110.03239},
  eprinttype = {arXiv},
  eprint    = {2110.03239},
  timestamp = {Tue, 30 Aug 2022 08:20:02 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2110-03239.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{Hsu.2023,
  title = {Sim-to-Lab-to-Real: Safe reinforcement learning with shielding and generalization guarantees},
  journal = {Artificial Intelligence},
  volume = {314},
  pages = {103811},
  year = {2023},
  issn = {0004-3702},
  doi = {https://doi.org/10.1016/j.artint.2022.103811},
  url = {https://www.sciencedirect.com/science/article/pii/S0004370222001515},
  author = {Kai-Chieh Hsu and Allen Z. Ren and Duy P. Nguyen and Anirudha Majumdar and Jaime F. Fisac},
  keywords = {Reinforcement learning, Sim-to-Real transfer, Safety analysis, Generalization},
  abstract = {Safety is a critical component of autonomous systems and remains a challenge for learning-based policies to be utilized in the real world. In particular, policies learned using reinforcement learning often fail to generalize to novel environments due to unsafe behavior. In this paper, we propose Sim-to-Lab-to-Real to bridge the reality gap with a probabilistically guaranteed safety-aware policy distribution. To improve safety, we apply a dual policy setup where a performance policy is trained using the cumulative task reward and a backup (safety) policy is trained by solving the Safety Bellman Equation based on Hamilton-Jacobi (HJ) reachability analysis. In Sim-to-Lab transfer, we apply a supervisory control scheme to shield unsafe actions during exploration; in Lab-to-Real transfer, we leverage the Probably Approximately Correct (PAC)-Bayes framework to provide lower bounds on the expected performance and safety of policies in unseen environments. Additionally, inheriting from the HJ reachability analysis, the bound accounts for the expectation over the worst-case safety in each environment. We empirically study the proposed framework for ego-vision navigation in two types of indoor environments with varying degrees of photorealism. We also demonstrate strong generalization performance through hardware experiments in real indoor spaces with a quadrupedal robot. See https://sites.google.com/princeton.edu/sim-to-lab-to-real for supplementary material.}
}

@misc{Panerati.332021,
  abstract = {Robotic simulators are crucial for academic research and education as well as the development of safety-critical applications. Reinforcement learning environments -- simple simulations coupled with a problem specification in the form of a reward function -- are also important to standardize the development (and benchmarking) of learning algorithms. Yet, full-scale simulators typically lack portability and parallelizability. Vice versa, many reinforcement learning environments trade-off realism for high sample throughputs in toy-like problems. While public data sets have greatly benefited deep learning and computer vision, we still lack the software tools to simultaneously develop -- and fairly compare -- control theory and reinforcement learning approaches. In this paper, we propose an open-source OpenAI Gym-like environment for multiple quadcopters based on the Bullet physics engine. Its multi-agent and vision based reinforcement learning interfaces, as well as the support of realistic collisions and aerodynamic effects, make it, to the best of our knowledge, a first of its kind. We demonstrate its use through several examples, either for control (trajectory tracking with PID control, multi-robot flight with downwash, etc.) or reinforcement learning (single and multi-agent stabilization tasks), hoping to inspire future research that combines control theory and machine learning.},
  author   = {Panerati, Jacopo and Zheng, Hehui and Zhou, SiQi and Xu, James and Prorok, Amanda and Schoellig, Angela P.},
  date     = {3/3/2021},
  year     = {2021},
  title    = {Learning to Fly -- a Gym Environment with PyBullet Physics for  Reinforcement Learning of Multi-agent Quadcopter Control},
  url      = {https://arxiv.org/pdf/2103.02142},
  keywords = {Computer Science - Learning;Computer Science - Robotics;Machine Learning (cs.LG);Robotics (cs.RO)},
  file     = {Learning to Fly -- a Gym Environment with PyBullet Physi:Attachments/Learning to Fly -- a Gym Environment with PyBullet Physi.pdf:application/pdf}
}

@article{Pinto.2017,
  author    = {Lerrel Pinto and
               James Davidson and
               Rahul Sukthankar and
               Abhinav Gupta},
  title     = {Robust Adversarial Reinforcement Learning},
  journal   = {CoRR},
  volume    = {abs/1703.02702},
  year      = {2017},
  url       = {http://arxiv.org/abs/1703.02702},
  eprinttype = {arXiv},
  eprint    = {1703.02702},
  timestamp = {Fri, 05 Apr 2019 07:29:46 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/PintoDSG17.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@book{Preiss,
  title     = {Entwurf und Verarbeitung relationaler Datenbanken},
  author    = {Preiß, N.},
  year      = {2007},
  publisher = {Oldenbourg},
  address   = {München/Wien},
  owner     = {tobiasstraub},
  timestamp = {2015.01.04}
}

@article{Primaerquelle,
  author   = {Originalautor},
  title    = {Originalliteratur, die nicht direkt zitiert wird und auch nicht im Verzeichnis erscheinen soll},
  year     = {2000},
  journal  = {Journal of Fake Sciences},
  keywords = {ausblenden}
}

@book{Recker.2021,
  author    = {Recker, Jan},
  year      = {2021},
  title     = {Scientific research in information systems: A beginner's guide},
  url       = {https://ebookcentral.proquest.com/lib/kxp/detail.action?docID=6789173},
  keywords  = {Electronic books},
  address   = {Cham},
  edition   = {Second Edition},
  publisher = {{Springer International Publishing}},
  isbn      = {9783030854362},
  series    = {Progress in IS},
  file      = {Recker Forschungsmethoden:Attachments/Recker Forschungsmethoden.pdf:application/pdf}
}

@inproceedings{Reda.2020,
  author    = {Reda, Daniele and Tao, Tianxin and {van de Panne}, Michiel},
  title     = {Learning to Locomote: Understanding How Environment Design Matters for Deep Reinforcement Learning},
  pages     = {1--10},
  publisher = {ACM},
  editor    = {Reda, Daniele and Tao, Tianxin and {van de Panne}, Michiel},
  booktitle = {Motion, Interaction and Games},
  year      = {2020},
  address   = {New York, NY, USA},
  doi       = {10.1145/3424636.3426907}
}

@proceedings{Reda.2020b,
  year      = {2020},
  title     = {Motion, Interaction and Games},
  address   = {New York, NY, USA},
  publisher = {ACM},
  editor    = {Reda, Daniele and Tao, Tianxin and {van de Panne}, Michiel},
  file      = {Learning to Locomote Understanding How Environment Desig:Attachments/Learning to Locomote Understanding How Environment Desig.pdf:application/pdf}
}

@article{Sadeghi.2016,
 author    = {Fereshteh Sadeghi and Sergey Levine},
 title     = {CAD2RL: Real Single-Image Flight without a Single Real Image},
 journal   = {CoRR},
 volume    = {abs/1611.04201},
 year      = {2016},
 url       = {http://arxiv.org/abs/1611.04201},
 eprinttype = {arXiv},
 eprint    = {1611.04201},
 timestamp = {Mon, 13 Aug 2018 16:46:31 +0200},
 biburl    = {https://dblp.org/rec/journals/corr/SadeghiL16.bib},
 bibsource = {dblp computer science bibliography, https://dblp.org}
}

@online{SAP:HANA,
  title         = {Real-time Analysis of Complaints for Life Sciences},
  author        = {{SAP AG}},
  url           = {http://hana.sap.com/abouthana/why-hana/usecases/real-time-analysis-complaints-life-sciences.html},
  year          = {o.J.},
  urldate       = {2015-01-05},
  date-modified = {2015-01-14 22:18:32 +0100},
  owner         = {tobiasstraub},
  timestamp     = {2015.01.05}
}

@book{Schlosser,
  title      = {Wissenschaftliche Arbeiten schreiben mit LATEX: Leitfaden für Einsteiger},
  author     = {Schlosser, Joachim},
  year       = {2014},
  edition    = {5., überarb. Aufl.},
  isbn       = {978-3-8266-9486-8},
  publisher  = {mitp-Verlag},
  bdsk-url-1 = {http://deposit.d-nb.de/cgi-bin/dokserv?id=4527943&prov=M&dok_var=1&dok_ext=htm}
}

@INPROCEEDINGS{Schott.2022,
  author={Schott, Lucas and Hajri, Hatem and Lamprier, Sylvain},
  booktitle={2022 International Joint Conference on Neural Networks (IJCNN)},
  title={Improving Robustness of Deep Reinforcement Learning Agents: Environment Attack based on the Critic Network},
  year={2022},
  volume={},
  number={},
  pages={1-8},
  doi={10.1109/IJCNN55064.2022.9892901}
}

@inproceedings{Schuderer.2021,
  abstract  = {Reinforcement learning (RL) is being used to create self-adaptive agents, where RL researchers commonly create and employ simulations of the problem domains to train and evaluate various RL algorithms and their variants. This activity is in need of methodological and...},
  author    = {Schuderer, Andreas and Bromuri, Stefano and {van Eekelen}, Marko},
  title     = {Sim-Env: Decoupling OpenAI Gym Environments from Simulation Models},
  url       = {https://link.springer.com/chapter/10.1007/978-3-030-85739-4_39},
  pages     = {390--393},
  publisher = {{Springer, Cham}},
  booktitle = {International Conference on Practical Applications of Agents and Multi-Agent Systems},
  year      = {2021},
  doi       = {10.1007/978-3-030-85739-4{\textunderscore }39}
}

@article{Schulman.2015,
  author    = {John Schulman and
               Sergey Levine and
               Philipp Moritz and
               Michael I. Jordan and
               Pieter Abbeel},
  title     = {Trust Region Policy Optimization},
  journal   = {CoRR},
  volume    = {abs/1502.05477},
  year      = {2015},
  url       = {http://arxiv.org/abs/1502.05477},
  eprinttype = {arXiv},
  eprint    = {1502.05477},
  timestamp = {Mon, 13 Aug 2018 16:48:08 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/SchulmanLMJA15.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{Schulman.2017,
  author    = {John Schulman and
               Filip Wolski and
               Prafulla Dhariwal and
               Alec Radford and
               Oleg Klimov},
  title     = {Proximal Policy Optimization Algorithms},
  journal   = {CoRR},
  volume    = {abs/1707.06347},
  year      = {2017},
  url       = {http://arxiv.org/abs/1707.06347},
  eprinttype = {arXiv},
  eprint    = {1707.06347},
  timestamp = {Mon, 13 Aug 2018 16:47:34 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/SchulmanWDRK17.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@book{Sekundaerquelle,
  author = {Sekundärautor},
  title  = {Sekundärliteratur wird tatsächlich ins Verzeichnis aufgenommen},
  year   = {2018}
}

@article{Shah.2017,
  author    = {Shital Shah and
               Debadeepta Dey and
               Chris Lovett and
               Ashish Kapoor},
  title     = {AirSim: High-Fidelity Visual and Physical Simulation for Autonomous
               Vehicles},
  journal   = {CoRR},
  volume    = {abs/1705.05065},
  year      = {2017},
  url       = {http://arxiv.org/abs/1705.05065},
  eprinttype = {arXiv},
  eprint    = {1705.05065},
  timestamp = {Mon, 13 Aug 2018 16:46:28 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/ShahDLK17.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@Inbook{Silano.2020,
 author="Silano, Giuseppe
 and Iannelli, Luigi",
 editor="Koubaa, Anis",
 title="CrazyS: A Software-in-the-Loop Simulation Platform for the Crazyflie 2.0 Nano-Quadcopter",
 bookTitle="Robot Operating System (ROS): The Complete Reference (Volume 4)",
 year="2019",
 publisher="Springer International Publishing",
 address="Cham",
 pages="81--115",
 abstract="This chapter proposes a typical use case dealing with the physical simulation of autonomous robots (specifically, quadrotors) and their interfacing through ROS (Robot Operating System). In particular, we propose CrazyS, an extension of the ROS package RotorS, aimed to modeling, developing and integrating the Crazyflie 2.0 nano-quadcopter in the physics based simulation environment Gazebo. Such simulation platform allows to understand quickly the behavior of the flight control system by comparing and evaluating different indoor and outdoor scenarios, with a details level quite close to reality. The proposed extension, running on Kinetic Kame ROS version but fully compatible with the Indigo Igloo one, expands the RotorS capabilities by considering the Crazyflie 2.0 physical model, its flight control system and the Crazyflie's on-board IMU, as well. A simple case study has been considered in order to show how the package works and how the dynamical model interacts with the control architecture of the quadcopter. The contribution can be also considered as a reference guide for expanding the RotorS functionalities in the UAVs field, by facilitating the integration of new aircrafts. We rel5,eased the software as open-source code, thus making it available for scientific and educational activities.",
 isbn="978-3-030-20190-6",
 doi="10.1007/978-3-030-20190-6_4",
 url="https://doi.org/10.1007/978-3-030-20190-6_4"
}

@article{Silva.2019,
  author  = {Silva, Felipe Leno Da and Costa, Anna Helena Reali},
  year    = {2019},
  title   = {A Survey on Transfer Learning for Multiagent Reinforcement Learning Systems},
  pages   = {645--703},
  volume  = {64},
  journal = {Journal of Artificial Intelligence Research},
  doi     = {10.1613/jair.1.11396},
  file    = {A Survey on Transfer Learning for Multiagent Reinforceme:Attachments/A Survey on Transfer Learning for Multiagent Reinforceme.pdf:application/pdf}
}

@book{Staab,
  title     = {Logik und Algebra: eine praxisbezogene Einführung für Informatiker und Wirtschaftsinformatiker},
  author    = {Staab, Frank},
  year      = {2012},
  edition   = {2., überarb. Aufl.},
  isbn      = {978-3-486-71697-9},
  pages     = {148},
  publisher = {Oldenbourg},
  address   = {München}
}

@article{Stoi,
  title     = {Management und Controlling von Intangibles},
  author    = {Stoi, R.},
  year      = {2003},
  number    = {1},
  pages     = {34--46},
  volume    = {4},
  journal   = {Studium \& Praxis},
  owner     = {tobiasstraub},
  timestamp = {2015.01.04}
}

@book{Sutton.2018,
  abstract  = {{\textless}b{\textgreater}The significantly expanded and updated new edition of a widely used text on reinforcement learning, one of the most active research areas in artificial intelligence.{\textless}/b{\textgreater}
               Reinforcement learning, one of the most active research areas in artificial intelligence, is a computational approach to learning whereby an agent tries to maximize the total amount of reward it receives while interacting with a complex, uncertain environment. In {\textless}i{\textgreater}Reinforcement Learning{\textless}/i{\textgreater}, Richard Sutton and Andrew Barto provide a clear and simple account of the field's key ideas and algorithms. This second edition has been significantly expanded and updated, presenting new topics and updating coverage of other topics.{\textless}/p{\textgreater}
               Like the first edition, this second edition focuses on core online learning algorithms, with the more mathematical material set off in shaded boxes. Part I covers as much of reinforcement learning as possible without going beyond the tabular case for which exact solutions can be found. Many algorithms presented in this part are new to the second edition, including UCB, Expected Sarsa, and Double Learning. Part II extends these ideas to function approximation, with new sections on such topics as artificial neural networks and the Fourier basis, and offers expanded treatment of off-policy learning and policy-gradient methods. Part III has new chapters on reinforcement learning's relationships to psychology and neuroscience, as well as an updated case-studies chapter including AlphaGo and AlphaGo Zero, Atari game playing, and IBM Watson's wagering strategy. The final chapter discusses the future societal impacts of reinforcement learning.{\textless}/p{\textgreater}},
  author    = {Sutton, Richard S. and Barto, Andrew G.},
  year      = {2018},
  title     = {Reinforcement Learning, second edition: An Introduction},
  publisher = {{MIT Press}},
  isbn      = {9780262352703}
}

@INPROCEEDINGS{Todorov.2012,
  author={Todorov, Emanuel and Erez, Tom and Tassa, Yuval},
  booktitle={2012 IEEE/RSJ International Conference on Intelligent Robots and Systems}, 
  title={MuJoCo: A physics engine for model-based control}, 
  year={2012},
  volume={},
  number={},
  pages={5026-5033},
  doi={10.1109/IROS.2012.6386109}
}

@inproceedings{Trautwein:Erfolgsfaktoren,
  title     = {Erfolgsfaktoren beim internationalen Planspieleinsatz},
  author    = {Friedrich Trautwein and Christina Trautwein},
  booktitle = {17. TOPSIM-Anwendertreffen},
  editor    = {{Tata Interactive Systems GmbH}},
  year      = {2008},
  owner     = {tobiasstraub},
  timestamp = {2015.01.04}
}

@inbook{Trautwein:Nokia,
  title     = {Nokia kämpft um die Vorherrschaft: Analyse der Mobiltelefonbranche},
  author    = {Friedrich Trautwein and Christina Trautwein},
  editor    = {R. Dillerup and R. Stoi},
  pages     = {81-84},
  publisher = {Vahlen Verlag},
  year      = {2008},
  address   = {München},
  booktitle = {Praxis der Unternehmensführung},
  owner     = {tobiasstraub},
  timestamp = {2015.01.04}
}

@article{trautwein2004berufliche,
  title   = {Berufliche Handlungskompetenz als Studienziel},
  author  = {Trautwein, Friedrich},
  year    = {2004},
  journal = {Hohenheim: Verlag Wissenschaft und Praxis}
}

@article{trautwein2011unternehmensplanspiele,
  title   = {Unternehmensplanspiele im industriebetrieblichen Hochschulstudium},
  author  = {Trautwein, Christina},
  year    = {2011},
  volume  = {147},
  journal = {Analyse von Kompetenzerwerb, Motivation und Zufriedenheit am Beispiel des Unternehmensplanspiels TOPSIM--General Management II. Wiesbaden: Gabler}
}

@misc{Umlauttest,
  title         = {Umlaute: Äöüß},
  author        = {äöüß€},
  howpublished  = {nur ein Test},
  date-modified = {2015-01-14 22:02:12 +0100}
}

@misc{Vuong.2019,
  abstract = {Recently, reinforcement learning (RL) algorithms have demonstrated remarkable success in learning complicated behaviors from minimally processed input. However, most of this success is limited to simulation. While there are promising successes in applying RL algorithms directly on real systems, their performance on more complex systems remains bottle-necked by the relative data inefficiency of RL algorithms. Domain randomization is a promising direction of research that has demonstrated impressive results using RL algorithms to control real robots. At a high level, domain randomization works by training a policy on a distribution of environmental conditions in simulation. If the environments are diverse enough, then the policy trained on this distribution will plausibly generalize to the real world. A human-specified design choice in domain randomization is the form and parameters of the distribution of simulated environments. It is unclear how to the best pick the form and parameters of this distribution and prior work uses hand-tuned distributions. This extended abstract demonstrates that the choice of the distribution plays a major role in the performance of the trained policies in the real world and that the parameter of this distribution can be optimized to maximize the performance of the trained policies in the real world},
  author   = {Vuong, Quan and Vikram, Sharad and Su, Hao and Gao, Sicun and Christensen, Henrik I.},
  date     = {2019},
  title    = {How to pick the domain randomization parameters for sim-to-real transfer of reinforcement learning policies?},
  url      = {https://arxiv.org/pdf/1903.11774},
  keywords = {Artificial Intelligence (cs.AI);Machine Learning (cs.LG);Machine Learning (stat.ML)},
  file     = {How to pick the domain randomization parameters for sim-:Attachments/How to pick the domain randomization parameters for sim-.pdf:application/pdf}
}


@misc{W3SchoolUnderscore,
  author   = {W3},
  keywords = {html,w3},
  pages    = {1},
  title    = {{HTML Semantic Elements}},
  url      = {https://www.w3schools.com/html/html5{\_}semantic{\_}elements.asp},
  urldate  = {2020-11-28},
  year     = {2020}
}


@article{Wang.2020,
  author  = {Wang, Zhe and Hong, Tianzhen},
  year    = {2020},
  title   = {Reinforcement learning for building controls: The opportunities and challenges},
  pages   = {115036},
  volume  = {269},
  issn    = {0306-2619},
  journal = {Applied Energy},
  doi     = {10.1016/j.apenergy.2020.115036},
  file    = {Reinforcement Learning for building controls:Attachments/Reinforcement Learning for building controls.pdf:application/pdf}
}


@online{wiki:Wirtschaftsinformatik,
  title      = {Wirtschaftsinformatik --- Wikipedia{,} Die freie Enzyklopädie},
  author     = {Wikipedia},
  url        = {http://de.wikipedia.org/w/index.php?title=Wirtschaftsinformatik&oldid=130413078},
  year       = {o.J.},
  urldate    = {2014-05-14},
  bdsk-url-1 = {http://de.wikipedia.org/w/index.php?title=Wirtschaftsinformatik&oldid=130413078}
}


@manual{Win8,
  title      = {Produkthandbuch für Windows 8 und Windows RT},
  author     = {Microsoft},
  year       = {2012},
  edition    = {Version 1.0},
  bdsk-url-1 = {https://www.microsoft.com/de-de/download/details.aspx?id=35406},
  owner      = {tobiasstraub},
  timestamp  = {2015.01.05}
}

@inbook{Wind,
  title     = {Cloud Management mit Open-Source-Plattformen},
  author    = {Stefan Wind},
  editor    = {Susanne Strahringer},
  publisher = {dpunkt.verlag GmbH},
  year      = {2012},
  booktitle = {Open Source - Konzepte, Risiken, Trends}
}


@article{Wong.2022,
  author  = {Wong, Annie and B{\"a}ck, Thomas and Kononova, Anna V. and Plaat, Aske},
  year    = {2022},
  title   = {Deep multiagent reinforcement learning: challenges and directions},
  issn    = {0269-2821},
  journal = {Artificial Intelligence Review},
  doi     = {10.1007/s10462-022-10299-x},
  file    = {Deep multiagent reinforcement learning challenges and di:Attachments/Deep multiagent reinforcement learning challenges and di.pdf:application/pdf}
}


@article{YanDuan.2016,
  abstract = {Benchmarking Deep Reinforcement Learning for Continuous ControlYan Duan,~Xi Chen,~Rein Houthooft,~John Schulman,~Pieter AbbeelRecently, r...},
  author   = {{Yan Duan} and {Xi Chen} and {Rein Houthooft} and {John Schulman} and {Pieter Abbeel}},
  year     = {2016},
  title    = {Benchmarking Deep Reinforcement Learning for Continuous Control},
  url      = {https://proceedings.mlr.press/v48/duan16.html},
  pages    = {1329--1338},
  issn     = {1938-7228},
  journal  = {International Conference on Machine Learning},
  file     = {Benchmarking Deep Reinforcement Learning for Continuous:Attachments/Benchmarking Deep Reinforcement Learning for Continuous.pdf:application/pdf}
}


@ARTICLE{Zhai.2022,
  author={Zhai, Peng and Hou, Taixian and Ji, Xiaopeng and Dong, Zhiyan and Zhang, Lihua},
  journal={IEEE Robotics and Automation Letters}, 
  title={Robust Adaptive Ensemble Adversary Reinforcement Learning}, 
  year={2022},
  volume={7},
  number={4},
  pages={12562-12568},
  doi={10.1109/LRA.2022.3220531}
}

@misc{Zhang.2018,
  doi       = {10.48550/ARXIV.1811.06032},
  url       = {https://arxiv.org/abs/1811.06032},
  author    = {Zhang, Amy and Wu, Yuxin and Pineau, Joelle},
  keywords  = {Machine Learning (cs.LG), Artificial Intelligence (cs.AI), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title     = {Natural Environment Benchmarks for Reinforcement Learning},
  publisher = {arXiv},
  year      = {2018},
  copyright = {arXiv.org perpetual, non-exclusive license}
}


@incollection{Zhao.,
  author = {Zhao, Wenshuai and Queralta, Jorge Pena and Westerlund, Tomi},
  title  = {Sim-to-Real Transfer in Deep Reinforcement Learning for Robotics: a Survey},
  pages  = {737--744},
  doi    = {10.1109/SSCI47803.2020.9308468},
  file   = {Sim-to-Real Transfer in Deep Reinforcement Learning for:Attachments/Sim-to-Real Transfer in Deep Reinforcement Learning for.pdf:application/pdf}
}


@inproceedings{Zhao.2020,
  author    = {Zhao, Wenshuai and Queralta, Jorge Pena and Westerlund, Tomi},
  title     = {Sim-to-Real Transfer in Deep Reinforcement Learning for Robotics: a Survey},
  publisher = {IEEE},
  booktitle = {2020 IEEE Symposium Series on Computational Intelligence (SSCI)},
  year      = {2020},
  doi       = {10.1109/ssci47803.2020.9308468}
}


@misc{Zhao.6272019,
  abstract = {With the recent advances in Reinforcement Learning (RL), there have been tremendous interests in employing RL for recommender systems. However, directly training and evaluating a new RL-based recommendation algorithm needs to collect users' real-time feedback in the real system, which is time and efforts consuming and could negatively impact on users' experiences. Thus, it calls for a user simulator that can mimic real users' behaviors where we can pre-train and evaluate new recommendation algorithms. Simulating users' behaviors in a dynamic system faces immense challenges -- (i) the underlining item distribution is complex, and (ii) historical logs for each user are limited. In this paper, we develop a user simulator base on Generative Adversarial Network (GAN). To be specific, the generator captures the underlining distribution of users' historical logs and generates realistic logs that can be considered as augmentations of real logs; while the discriminator not only distinguishes real and fake logs but also predicts users' behaviors. The experimental results based on real-world e-commerce data demonstrate the effectiveness of the proposed simulator.},
  author   = {Zhao, Xiangyu and Xia, Long and Zou, Lixin and Yin, Dawei and Tang, Jiliang},
  date     = {6/27/2019},
  title    = {Toward Simulating Environments in Reinforcement Learning Based  Recommendations},
  url      = {https://arxiv.org/pdf/1906.11462},
  keywords = {Computer Science - Information Retrieval;Computer Science - Learning;Information Retrieval (cs.IR);Machine Learning (cs.LG)},
  file     = {Toward Simulating Environments in Reinforcement Learning:Attachments/Toward Simulating Environments in Reinforcement Learning.pdf:application/pdf}
}


@inproceedings{Zubow.2021,
  author    = {Zubow, Anatolij and R{\"o}sler, Sascha and Gaw{\l}owicz, Piotr and Dressler, Falko},
  title     = {GrGym},
  publisher = {ACM},
  booktitle = {Proceedings of the 22nd International Workshop on Mobile Computing Systems and Applications},
  year      = {2021},
  address   = {New York, NY, USA},
  doi       = {10.1145/3446382.3448358}
}

@Comment{jabref-meta: databaseType:biblatex;}
